{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home Credit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_selection import  RFE, RFECV, SelectFromModel\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, f1_score, precision_score, auc, roc_curve\n",
    "import seaborn as sns\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import time \n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureSelectionExecuter(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self,instclass,withy):\n",
    "        self.a=instclass\n",
    "        self.b=withy\n",
    "        \n",
    "    def fit(self,X,y,*_):\n",
    "        if self.b == \"No\":\n",
    "            self.a.fit(X)\n",
    "        else:\n",
    "            self.a.fit(X,y)\n",
    "        return self\n",
    "    \n",
    "    def transform(self,Z,*_): \n",
    "        return pd.DataFrame(self.a.transform(Z), columns=Z.columns[self.a.get_support()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelmetrics(name,y_pred,y_pred_proba,y_real,time_model):\n",
    "    \n",
    "    df_score = pd.concat([pd.DataFrame(data=y_pred_proba[:,1], columns=['score']),\n",
    "                       pd.DataFrame(data=y_real)], axis=1)\n",
    "    df_score = df_score.sort_values('score',ascending=False).reset_index(drop=True)\n",
    "    df_score['bin'] = pd.qcut(df_score.index, 8)\n",
    "    score = df_score.groupby(by='bin').agg({'score':{'score_min':min,'score_max':max}, 'TARGET':{'bads':sum,'total':'count'}})\n",
    "    score=score.reset_index(level=0, col_fill='bin')\n",
    "    score.columns = score.columns.droplevel(level=0)\n",
    "    score['goods']=score['total']-score['bads']\n",
    "    score['bad_rate']=score['bads']/score['total']\n",
    "    score['bads_cum']=score['bads'].cumsum()\n",
    "    score['goods_cum']=score['goods'].cumsum()\n",
    "    score['total_cum']=score['total'].cumsum()\n",
    "    score['bads_cum_rate']=score['bads_cum']/score['bads'].sum()\n",
    "    score['goods_cum_rate']=score['goods_cum']/score['goods'].sum()\n",
    "    score['total_cum_rate']=score['total_cum']/score['total'].sum()\n",
    "    score['KS']=score['bads_cum_rate']-score['goods_cum_rate']\n",
    "    ks=score['KS'].max()\n",
    "    \n",
    "    cm = confusion_matrix(y_real, y_pred)\n",
    "    fpr,tpr,umbral = roc_curve(y_real, y_pred_proba[:,1])\n",
    "    d = {'TP':cm[1][1],\n",
    "         'TN':cm[0][0],\n",
    "         'FP':cm[0][1],\n",
    "         'FN':cm[1][0],\n",
    "         'Accuracy':accuracy_score(y_real, y_pred),\n",
    "         'Recall':recall_score(y_real, y_pred),\n",
    "         'Precision':precision_score(y_real, y_pred),\n",
    "         'F1':f1_score(y_real, y_pred),\n",
    "         'AUC':auc(fpr,tpr),\n",
    "         'Gini':2*auc(fpr,tpr)-1,\n",
    "         'KS':ks, \n",
    "         'Minutes':time_model}\n",
    "    df = pd.DataFrame(d, index=[name])\n",
    "    df=df.reindex(columns=['TP','TN','FP','FN','Accuracy','Recall','Precision','F1','AUC','Gini','KS','Minutes'])\n",
    "    \n",
    "    return score, df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(246008, 187)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(61502, 187)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(246008, 1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(61502, 1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import data sets\n",
    "X_train_2 = pd.read_csv('X_train_2.csv', index_col=0)\n",
    "X_test_2 = pd.read_csv('X_test_2.csv', index_col=0)\n",
    "y_train_2 = pd.read_csv('y_train_2.csv', header=None)\n",
    "y_test_2 = pd.read_csv('y_test_2.csv', header=None)\n",
    "\n",
    "y_train_2 = y_train_2.drop(columns=0).rename(columns={1:'TARGET'})\n",
    "y_test_2 = y_test_2.drop(columns=0).rename(columns={1:'TARGET'})\n",
    "\n",
    "display(X_train_2.shape, X_test_2.shape, y_train_2.shape, y_test_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_compare = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Logistic Regression (NO FS NO SM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pipeline_LR = make_pipeline(LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TP</th>\n",
       "      <th>TN</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Gini</th>\n",
       "      <th>KS</th>\n",
       "      <th>Minutes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>36</td>\n",
       "      <td>56497</td>\n",
       "      <td>40</td>\n",
       "      <td>4929</td>\n",
       "      <td>0.919206</td>\n",
       "      <td>0.007251</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.014283</td>\n",
       "      <td>0.741644</td>\n",
       "      <td>0.483288</td>\n",
       "      <td>0.361981</td>\n",
       "      <td>0.131119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    TP     TN  FP    FN  Accuracy    Recall  Precision  \\\n",
       "LogisticRegression  36  56497  40  4929  0.919206  0.007251   0.473684   \n",
       "\n",
       "                          F1       AUC      Gini        KS   Minutes  \n",
       "LogisticRegression  0.014283  0.741644  0.483288  0.361981  0.131119  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bin</th>\n",
       "      <th>score_min</th>\n",
       "      <th>score_max</th>\n",
       "      <th>bads</th>\n",
       "      <th>total</th>\n",
       "      <th>goods</th>\n",
       "      <th>bad_rate</th>\n",
       "      <th>bads_cum</th>\n",
       "      <th>goods_cum</th>\n",
       "      <th>total_cum</th>\n",
       "      <th>bads_cum_rate</th>\n",
       "      <th>goods_cum_rate</th>\n",
       "      <th>total_cum_rate</th>\n",
       "      <th>KS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>(-0.001, 7687.625]</td>\n",
       "      <td>0.154064</td>\n",
       "      <td>0.672522</td>\n",
       "      <td>1748</td>\n",
       "      <td>7688</td>\n",
       "      <td>5940</td>\n",
       "      <td>0.227367</td>\n",
       "      <td>1748</td>\n",
       "      <td>5940</td>\n",
       "      <td>7688</td>\n",
       "      <td>0.352064</td>\n",
       "      <td>0.105064</td>\n",
       "      <td>0.125004</td>\n",
       "      <td>0.247001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>(7687.625, 15375.25]</td>\n",
       "      <td>0.102452</td>\n",
       "      <td>0.154051</td>\n",
       "      <td>1035</td>\n",
       "      <td>7688</td>\n",
       "      <td>6653</td>\n",
       "      <td>0.134625</td>\n",
       "      <td>2783</td>\n",
       "      <td>12593</td>\n",
       "      <td>15376</td>\n",
       "      <td>0.560524</td>\n",
       "      <td>0.222739</td>\n",
       "      <td>0.250008</td>\n",
       "      <td>0.337785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>(15375.25, 23062.875]</td>\n",
       "      <td>0.075157</td>\n",
       "      <td>0.102446</td>\n",
       "      <td>731</td>\n",
       "      <td>7687</td>\n",
       "      <td>6956</td>\n",
       "      <td>0.095096</td>\n",
       "      <td>3514</td>\n",
       "      <td>19549</td>\n",
       "      <td>23063</td>\n",
       "      <td>0.707754</td>\n",
       "      <td>0.345774</td>\n",
       "      <td>0.374996</td>\n",
       "      <td>0.361981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>(23062.875, 30750.5]</td>\n",
       "      <td>0.057100</td>\n",
       "      <td>0.075156</td>\n",
       "      <td>495</td>\n",
       "      <td>7688</td>\n",
       "      <td>7193</td>\n",
       "      <td>0.064386</td>\n",
       "      <td>4009</td>\n",
       "      <td>26742</td>\n",
       "      <td>30751</td>\n",
       "      <td>0.807452</td>\n",
       "      <td>0.473000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.334452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>(30750.5, 38438.125]</td>\n",
       "      <td>0.043729</td>\n",
       "      <td>0.057098</td>\n",
       "      <td>368</td>\n",
       "      <td>7688</td>\n",
       "      <td>7320</td>\n",
       "      <td>0.047867</td>\n",
       "      <td>4377</td>\n",
       "      <td>34062</td>\n",
       "      <td>38439</td>\n",
       "      <td>0.881571</td>\n",
       "      <td>0.602473</td>\n",
       "      <td>0.625004</td>\n",
       "      <td>0.279098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>(38438.125, 46125.75]</td>\n",
       "      <td>0.032831</td>\n",
       "      <td>0.043729</td>\n",
       "      <td>291</td>\n",
       "      <td>7687</td>\n",
       "      <td>7396</td>\n",
       "      <td>0.037856</td>\n",
       "      <td>4668</td>\n",
       "      <td>41458</td>\n",
       "      <td>46126</td>\n",
       "      <td>0.940181</td>\n",
       "      <td>0.733290</td>\n",
       "      <td>0.749992</td>\n",
       "      <td>0.206892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>(46125.75, 53813.375]</td>\n",
       "      <td>0.022683</td>\n",
       "      <td>0.032829</td>\n",
       "      <td>186</td>\n",
       "      <td>7688</td>\n",
       "      <td>7502</td>\n",
       "      <td>0.024194</td>\n",
       "      <td>4854</td>\n",
       "      <td>48960</td>\n",
       "      <td>53814</td>\n",
       "      <td>0.977644</td>\n",
       "      <td>0.865982</td>\n",
       "      <td>0.874996</td>\n",
       "      <td>0.111662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>(53813.375, 61501.0]</td>\n",
       "      <td>0.001559</td>\n",
       "      <td>0.022682</td>\n",
       "      <td>111</td>\n",
       "      <td>7688</td>\n",
       "      <td>7577</td>\n",
       "      <td>0.014438</td>\n",
       "      <td>4965</td>\n",
       "      <td>56537</td>\n",
       "      <td>61502</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     bin  score_min  score_max  bads  total  goods  bad_rate  \\\n",
       "0     (-0.001, 7687.625]   0.154064   0.672522  1748   7688   5940  0.227367   \n",
       "1   (7687.625, 15375.25]   0.102452   0.154051  1035   7688   6653  0.134625   \n",
       "2  (15375.25, 23062.875]   0.075157   0.102446   731   7687   6956  0.095096   \n",
       "3   (23062.875, 30750.5]   0.057100   0.075156   495   7688   7193  0.064386   \n",
       "4   (30750.5, 38438.125]   0.043729   0.057098   368   7688   7320  0.047867   \n",
       "5  (38438.125, 46125.75]   0.032831   0.043729   291   7687   7396  0.037856   \n",
       "6  (46125.75, 53813.375]   0.022683   0.032829   186   7688   7502  0.024194   \n",
       "7   (53813.375, 61501.0]   0.001559   0.022682   111   7688   7577  0.014438   \n",
       "\n",
       "   bads_cum  goods_cum  total_cum  bads_cum_rate  goods_cum_rate  \\\n",
       "0      1748       5940       7688       0.352064        0.105064   \n",
       "1      2783      12593      15376       0.560524        0.222739   \n",
       "2      3514      19549      23063       0.707754        0.345774   \n",
       "3      4009      26742      30751       0.807452        0.473000   \n",
       "4      4377      34062      38439       0.881571        0.602473   \n",
       "5      4668      41458      46126       0.940181        0.733290   \n",
       "6      4854      48960      53814       0.977644        0.865982   \n",
       "7      4965      56537      61502       1.000000        1.000000   \n",
       "\n",
       "   total_cum_rate        KS  \n",
       "0        0.125004  0.247001  \n",
       "1        0.250008  0.337785  \n",
       "2        0.374996  0.361981  \n",
       "3        0.500000  0.334452  \n",
       "4        0.625004  0.279098  \n",
       "5        0.749992  0.206892  \n",
       "6        0.874996  0.111662  \n",
       "7        1.000000  0.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "model_pipeline_LR.fit(X_train_2,y_train_2)\n",
    "y_pred = model_pipeline_LR.predict(X_test_2)\n",
    "y_pred_proba = model_pipeline_LR.predict_proba(X_test_2)\n",
    "\n",
    "end = time.time()\n",
    "time_model=end-start\n",
    "time_model=time_model/60\n",
    "\n",
    "score_LR, df_LR = modelmetrics('LogisticRegression', y_pred, y_pred_proba, y_test_2,time_model) \n",
    "\n",
    "model_compare= pd.concat([model_compare,df_LR])\n",
    "model_compare_LR = model_compare\n",
    "\n",
    "display(model_compare, score_LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Logistic Regression with feature selection RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_selection_LR_RFE_pipeline = make_pipeline(FeatureSelectionExecuter(RFE(LogisticRegression(max_iter = 1000),n_features_to_select=120),\"Yes\"))\n",
    "model_pipeline_LR_RFE = make_pipeline(LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TP</th>\n",
       "      <th>TN</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Gini</th>\n",
       "      <th>KS</th>\n",
       "      <th>Minutes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>36</td>\n",
       "      <td>56497</td>\n",
       "      <td>40</td>\n",
       "      <td>4929</td>\n",
       "      <td>0.919206</td>\n",
       "      <td>0.007251</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.014283</td>\n",
       "      <td>0.741644</td>\n",
       "      <td>0.483288</td>\n",
       "      <td>0.361981</td>\n",
       "      <td>0.131119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>LogisticRegression RFE</td>\n",
       "      <td>37</td>\n",
       "      <td>56499</td>\n",
       "      <td>38</td>\n",
       "      <td>4928</td>\n",
       "      <td>0.919255</td>\n",
       "      <td>0.007452</td>\n",
       "      <td>0.493333</td>\n",
       "      <td>0.014683</td>\n",
       "      <td>0.742399</td>\n",
       "      <td>0.484797</td>\n",
       "      <td>0.365048</td>\n",
       "      <td>41.746602</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        TP     TN  FP    FN  Accuracy    Recall  Precision  \\\n",
       "LogisticRegression      36  56497  40  4929  0.919206  0.007251   0.473684   \n",
       "LogisticRegression RFE  37  56499  38  4928  0.919255  0.007452   0.493333   \n",
       "\n",
       "                              F1       AUC      Gini        KS    Minutes  \n",
       "LogisticRegression      0.014283  0.741644  0.483288  0.361981   0.131119  \n",
       "LogisticRegression RFE  0.014683  0.742399  0.484797  0.365048  41.746602  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bin</th>\n",
       "      <th>score_min</th>\n",
       "      <th>score_max</th>\n",
       "      <th>bads</th>\n",
       "      <th>total</th>\n",
       "      <th>goods</th>\n",
       "      <th>bad_rate</th>\n",
       "      <th>bads_cum</th>\n",
       "      <th>goods_cum</th>\n",
       "      <th>total_cum</th>\n",
       "      <th>bads_cum_rate</th>\n",
       "      <th>goods_cum_rate</th>\n",
       "      <th>total_cum_rate</th>\n",
       "      <th>KS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>(-0.001, 7687.625]</td>\n",
       "      <td>0.153295</td>\n",
       "      <td>0.672430</td>\n",
       "      <td>1757</td>\n",
       "      <td>7688</td>\n",
       "      <td>5931</td>\n",
       "      <td>0.228538</td>\n",
       "      <td>1757</td>\n",
       "      <td>5931</td>\n",
       "      <td>7688</td>\n",
       "      <td>0.353877</td>\n",
       "      <td>0.104905</td>\n",
       "      <td>0.125004</td>\n",
       "      <td>0.248972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>(7687.625, 15375.25]</td>\n",
       "      <td>0.102078</td>\n",
       "      <td>0.153291</td>\n",
       "      <td>1053</td>\n",
       "      <td>7688</td>\n",
       "      <td>6635</td>\n",
       "      <td>0.136967</td>\n",
       "      <td>2810</td>\n",
       "      <td>12566</td>\n",
       "      <td>15376</td>\n",
       "      <td>0.565962</td>\n",
       "      <td>0.222262</td>\n",
       "      <td>0.250008</td>\n",
       "      <td>0.343700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>(15375.25, 23062.875]</td>\n",
       "      <td>0.074640</td>\n",
       "      <td>0.102069</td>\n",
       "      <td>718</td>\n",
       "      <td>7687</td>\n",
       "      <td>6969</td>\n",
       "      <td>0.093404</td>\n",
       "      <td>3528</td>\n",
       "      <td>19535</td>\n",
       "      <td>23063</td>\n",
       "      <td>0.710574</td>\n",
       "      <td>0.345526</td>\n",
       "      <td>0.374996</td>\n",
       "      <td>0.365048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>(23062.875, 30750.5]</td>\n",
       "      <td>0.056749</td>\n",
       "      <td>0.074638</td>\n",
       "      <td>487</td>\n",
       "      <td>7688</td>\n",
       "      <td>7201</td>\n",
       "      <td>0.063345</td>\n",
       "      <td>4015</td>\n",
       "      <td>26736</td>\n",
       "      <td>30751</td>\n",
       "      <td>0.808661</td>\n",
       "      <td>0.472894</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.335767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>(30750.5, 38438.125]</td>\n",
       "      <td>0.043495</td>\n",
       "      <td>0.056748</td>\n",
       "      <td>369</td>\n",
       "      <td>7688</td>\n",
       "      <td>7319</td>\n",
       "      <td>0.047997</td>\n",
       "      <td>4384</td>\n",
       "      <td>34055</td>\n",
       "      <td>38439</td>\n",
       "      <td>0.882981</td>\n",
       "      <td>0.602349</td>\n",
       "      <td>0.625004</td>\n",
       "      <td>0.280632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>(38438.125, 46125.75]</td>\n",
       "      <td>0.032807</td>\n",
       "      <td>0.043495</td>\n",
       "      <td>268</td>\n",
       "      <td>7687</td>\n",
       "      <td>7419</td>\n",
       "      <td>0.034864</td>\n",
       "      <td>4652</td>\n",
       "      <td>41474</td>\n",
       "      <td>46126</td>\n",
       "      <td>0.936959</td>\n",
       "      <td>0.733573</td>\n",
       "      <td>0.749992</td>\n",
       "      <td>0.203386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>(46125.75, 53813.375]</td>\n",
       "      <td>0.022691</td>\n",
       "      <td>0.032806</td>\n",
       "      <td>200</td>\n",
       "      <td>7688</td>\n",
       "      <td>7488</td>\n",
       "      <td>0.026015</td>\n",
       "      <td>4852</td>\n",
       "      <td>48962</td>\n",
       "      <td>53814</td>\n",
       "      <td>0.977241</td>\n",
       "      <td>0.866017</td>\n",
       "      <td>0.874996</td>\n",
       "      <td>0.111224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>(53813.375, 61501.0]</td>\n",
       "      <td>0.001805</td>\n",
       "      <td>0.022689</td>\n",
       "      <td>113</td>\n",
       "      <td>7688</td>\n",
       "      <td>7575</td>\n",
       "      <td>0.014698</td>\n",
       "      <td>4965</td>\n",
       "      <td>56537</td>\n",
       "      <td>61502</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     bin  score_min  score_max  bads  total  goods  bad_rate  \\\n",
       "0     (-0.001, 7687.625]   0.153295   0.672430  1757   7688   5931  0.228538   \n",
       "1   (7687.625, 15375.25]   0.102078   0.153291  1053   7688   6635  0.136967   \n",
       "2  (15375.25, 23062.875]   0.074640   0.102069   718   7687   6969  0.093404   \n",
       "3   (23062.875, 30750.5]   0.056749   0.074638   487   7688   7201  0.063345   \n",
       "4   (30750.5, 38438.125]   0.043495   0.056748   369   7688   7319  0.047997   \n",
       "5  (38438.125, 46125.75]   0.032807   0.043495   268   7687   7419  0.034864   \n",
       "6  (46125.75, 53813.375]   0.022691   0.032806   200   7688   7488  0.026015   \n",
       "7   (53813.375, 61501.0]   0.001805   0.022689   113   7688   7575  0.014698   \n",
       "\n",
       "   bads_cum  goods_cum  total_cum  bads_cum_rate  goods_cum_rate  \\\n",
       "0      1757       5931       7688       0.353877        0.104905   \n",
       "1      2810      12566      15376       0.565962        0.222262   \n",
       "2      3528      19535      23063       0.710574        0.345526   \n",
       "3      4015      26736      30751       0.808661        0.472894   \n",
       "4      4384      34055      38439       0.882981        0.602349   \n",
       "5      4652      41474      46126       0.936959        0.733573   \n",
       "6      4852      48962      53814       0.977241        0.866017   \n",
       "7      4965      56537      61502       1.000000        1.000000   \n",
       "\n",
       "   total_cum_rate        KS  \n",
       "0        0.125004  0.248972  \n",
       "1        0.250008  0.343700  \n",
       "2        0.374996  0.365048  \n",
       "3        0.500000  0.335767  \n",
       "4        0.625004  0.280632  \n",
       "5        0.749992  0.203386  \n",
       "6        0.874996  0.111224  \n",
       "7        1.000000  0.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "X_train_2_fs = feature_selection_LR_RFE_pipeline.fit_transform(X_train_2,y_train_2)\n",
    "X_test_2_fs = feature_selection_LR_RFE_pipeline.transform(X_test_2)\n",
    "\n",
    "\n",
    "model_pipeline_LR_RFE.fit(X_train_2_fs,y_train_2)\n",
    "y_pred = model_pipeline_LR_RFE.predict(X_test_2_fs)\n",
    "y_pred_proba = model_pipeline_LR_RFE.predict_proba(X_test_2_fs)\n",
    "\n",
    "end = time.time()\n",
    "time_model=end-start\n",
    "time_model=time_model/60\n",
    "\n",
    "score_LR_RFE, df_LR_RFE = modelmetrics('LogisticRegression RFE', y_pred, y_pred_proba, y_test_2,time_model) \n",
    "\n",
    "model_compare=pd.concat([model_compare,df_LR_RFE])\n",
    "model_compare_LR_RFE = model_compare\n",
    "\n",
    "display(model_compare, score_LR_RFE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Logistic Regression with feature selection RFECV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf=KFold(n_splits=3, shuffle=True, random_state=1234)\n",
    "feature_selection_LR_RFECV_pipeline = make_pipeline(FeatureSelectionExecuter\n",
    "                                                    (RFECV(LogisticRegression(max_iter = 4000),\n",
    "                                                    cv=kf,scoring='neg_mean_squared_error')\n",
    "                                                     ,\"Yes\"))\n",
    "model_pipeline_LR_RFECV = make_pipeline(LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TP</th>\n",
       "      <th>TN</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Gini</th>\n",
       "      <th>KS</th>\n",
       "      <th>Minutes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>36</td>\n",
       "      <td>56497</td>\n",
       "      <td>40</td>\n",
       "      <td>4929</td>\n",
       "      <td>0.919206</td>\n",
       "      <td>0.007251</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.014283</td>\n",
       "      <td>0.741644</td>\n",
       "      <td>0.483288</td>\n",
       "      <td>0.361981</td>\n",
       "      <td>0.131119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>LogisticRegression RFE</td>\n",
       "      <td>37</td>\n",
       "      <td>56499</td>\n",
       "      <td>38</td>\n",
       "      <td>4928</td>\n",
       "      <td>0.919255</td>\n",
       "      <td>0.007452</td>\n",
       "      <td>0.493333</td>\n",
       "      <td>0.014683</td>\n",
       "      <td>0.742399</td>\n",
       "      <td>0.484797</td>\n",
       "      <td>0.365048</td>\n",
       "      <td>41.746602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>LogisticRegression RFECV</td>\n",
       "      <td>0</td>\n",
       "      <td>56537</td>\n",
       "      <td>0</td>\n",
       "      <td>4965</td>\n",
       "      <td>0.919271</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.539789</td>\n",
       "      <td>0.079578</td>\n",
       "      <td>0.093007</td>\n",
       "      <td>189.196640</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          TP     TN  FP    FN  Accuracy    Recall  Precision  \\\n",
       "LogisticRegression        36  56497  40  4929  0.919206  0.007251   0.473684   \n",
       "LogisticRegression RFE    37  56499  38  4928  0.919255  0.007452   0.493333   \n",
       "LogisticRegression RFECV   0  56537   0  4965  0.919271  0.000000   0.000000   \n",
       "\n",
       "                                F1       AUC      Gini        KS     Minutes  \n",
       "LogisticRegression        0.014283  0.741644  0.483288  0.361981    0.131119  \n",
       "LogisticRegression RFE    0.014683  0.742399  0.484797  0.365048   41.746602  \n",
       "LogisticRegression RFECV  0.000000  0.539789  0.079578  0.093007  189.196640  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bin</th>\n",
       "      <th>score_min</th>\n",
       "      <th>score_max</th>\n",
       "      <th>bads</th>\n",
       "      <th>total</th>\n",
       "      <th>goods</th>\n",
       "      <th>bad_rate</th>\n",
       "      <th>bads_cum</th>\n",
       "      <th>goods_cum</th>\n",
       "      <th>total_cum</th>\n",
       "      <th>bads_cum_rate</th>\n",
       "      <th>goods_cum_rate</th>\n",
       "      <th>total_cum_rate</th>\n",
       "      <th>KS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>(-0.001, 7687.625]</td>\n",
       "      <td>0.091610</td>\n",
       "      <td>0.096346</td>\n",
       "      <td>578</td>\n",
       "      <td>7688</td>\n",
       "      <td>7110</td>\n",
       "      <td>0.075182</td>\n",
       "      <td>578</td>\n",
       "      <td>7110</td>\n",
       "      <td>7688</td>\n",
       "      <td>0.116415</td>\n",
       "      <td>0.125758</td>\n",
       "      <td>0.125004</td>\n",
       "      <td>-0.009343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>(7687.625, 15375.25]</td>\n",
       "      <td>0.089624</td>\n",
       "      <td>0.091610</td>\n",
       "      <td>669</td>\n",
       "      <td>7688</td>\n",
       "      <td>7019</td>\n",
       "      <td>0.087019</td>\n",
       "      <td>1247</td>\n",
       "      <td>14129</td>\n",
       "      <td>15376</td>\n",
       "      <td>0.251158</td>\n",
       "      <td>0.249907</td>\n",
       "      <td>0.250008</td>\n",
       "      <td>0.001251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>(15375.25, 23062.875]</td>\n",
       "      <td>0.086205</td>\n",
       "      <td>0.089624</td>\n",
       "      <td>665</td>\n",
       "      <td>7687</td>\n",
       "      <td>7022</td>\n",
       "      <td>0.086510</td>\n",
       "      <td>1912</td>\n",
       "      <td>21151</td>\n",
       "      <td>23063</td>\n",
       "      <td>0.385096</td>\n",
       "      <td>0.374109</td>\n",
       "      <td>0.374996</td>\n",
       "      <td>0.010987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>(23062.875, 30750.5]</td>\n",
       "      <td>0.082764</td>\n",
       "      <td>0.086205</td>\n",
       "      <td>995</td>\n",
       "      <td>7688</td>\n",
       "      <td>6693</td>\n",
       "      <td>0.129422</td>\n",
       "      <td>2907</td>\n",
       "      <td>27844</td>\n",
       "      <td>30751</td>\n",
       "      <td>0.585498</td>\n",
       "      <td>0.492492</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.093007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>(30750.5, 38438.125]</td>\n",
       "      <td>0.079042</td>\n",
       "      <td>0.082764</td>\n",
       "      <td>577</td>\n",
       "      <td>7688</td>\n",
       "      <td>7111</td>\n",
       "      <td>0.075052</td>\n",
       "      <td>3484</td>\n",
       "      <td>34955</td>\n",
       "      <td>38439</td>\n",
       "      <td>0.701712</td>\n",
       "      <td>0.618268</td>\n",
       "      <td>0.625004</td>\n",
       "      <td>0.083444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>(38438.125, 46125.75]</td>\n",
       "      <td>0.075863</td>\n",
       "      <td>0.079042</td>\n",
       "      <td>641</td>\n",
       "      <td>7687</td>\n",
       "      <td>7046</td>\n",
       "      <td>0.083388</td>\n",
       "      <td>4125</td>\n",
       "      <td>42001</td>\n",
       "      <td>46126</td>\n",
       "      <td>0.830816</td>\n",
       "      <td>0.742894</td>\n",
       "      <td>0.749992</td>\n",
       "      <td>0.087922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>(46125.75, 53813.375]</td>\n",
       "      <td>0.069135</td>\n",
       "      <td>0.075863</td>\n",
       "      <td>468</td>\n",
       "      <td>7688</td>\n",
       "      <td>7220</td>\n",
       "      <td>0.060874</td>\n",
       "      <td>4593</td>\n",
       "      <td>49221</td>\n",
       "      <td>53814</td>\n",
       "      <td>0.925076</td>\n",
       "      <td>0.870598</td>\n",
       "      <td>0.874996</td>\n",
       "      <td>0.054478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>(53813.375, 61501.0]</td>\n",
       "      <td>0.020063</td>\n",
       "      <td>0.069135</td>\n",
       "      <td>372</td>\n",
       "      <td>7688</td>\n",
       "      <td>7316</td>\n",
       "      <td>0.048387</td>\n",
       "      <td>4965</td>\n",
       "      <td>56537</td>\n",
       "      <td>61502</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     bin  score_min  score_max  bads  total  goods  bad_rate  \\\n",
       "0     (-0.001, 7687.625]   0.091610   0.096346   578   7688   7110  0.075182   \n",
       "1   (7687.625, 15375.25]   0.089624   0.091610   669   7688   7019  0.087019   \n",
       "2  (15375.25, 23062.875]   0.086205   0.089624   665   7687   7022  0.086510   \n",
       "3   (23062.875, 30750.5]   0.082764   0.086205   995   7688   6693  0.129422   \n",
       "4   (30750.5, 38438.125]   0.079042   0.082764   577   7688   7111  0.075052   \n",
       "5  (38438.125, 46125.75]   0.075863   0.079042   641   7687   7046  0.083388   \n",
       "6  (46125.75, 53813.375]   0.069135   0.075863   468   7688   7220  0.060874   \n",
       "7   (53813.375, 61501.0]   0.020063   0.069135   372   7688   7316  0.048387   \n",
       "\n",
       "   bads_cum  goods_cum  total_cum  bads_cum_rate  goods_cum_rate  \\\n",
       "0       578       7110       7688       0.116415        0.125758   \n",
       "1      1247      14129      15376       0.251158        0.249907   \n",
       "2      1912      21151      23063       0.385096        0.374109   \n",
       "3      2907      27844      30751       0.585498        0.492492   \n",
       "4      3484      34955      38439       0.701712        0.618268   \n",
       "5      4125      42001      46126       0.830816        0.742894   \n",
       "6      4593      49221      53814       0.925076        0.870598   \n",
       "7      4965      56537      61502       1.000000        1.000000   \n",
       "\n",
       "   total_cum_rate        KS  \n",
       "0        0.125004 -0.009343  \n",
       "1        0.250008  0.001251  \n",
       "2        0.374996  0.010987  \n",
       "3        0.500000  0.093007  \n",
       "4        0.625004  0.083444  \n",
       "5        0.749992  0.087922  \n",
       "6        0.874996  0.054478  \n",
       "7        1.000000  0.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "X_train_2_fs = feature_selection_LR_RFECV_pipeline.fit_transform(X_train_2,y_train_2)\n",
    "X_test_2_fs = feature_selection_LR_RFECV_pipeline.transform(X_test_2)\n",
    "\n",
    "\n",
    "model_pipeline_LR_RFECV.fit(X_train_2_fs,y_train_2)\n",
    "y_pred = model_pipeline_LR_RFECV.predict(X_test_2_fs)\n",
    "y_pred_proba = model_pipeline_LR_RFECV.predict_proba(X_test_2_fs)\n",
    "\n",
    "end = time.time()\n",
    "time_model=end-start\n",
    "time_model=time_model/60\n",
    "\n",
    "score_LR_RFECV, df_LR_RFECV = modelmetrics('LogisticRegression RFECV', y_pred, y_pred_proba, y_test_2,time_model) \n",
    "\n",
    "model_compare=pd.concat([model_compare,df_LR_RFECV])\n",
    "model_compare_LR_RFECV = model_compare\n",
    "\n",
    "display(model_compare, score_LR_RFECV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Logistic Regression with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#smote is not in a pipeline\n",
    "smote= SMOTE(random_state=101)\n",
    "model_pipeline_LR_SM = make_pipeline(LogisticRegression(max_iter = 4000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TP</th>\n",
       "      <th>TN</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Gini</th>\n",
       "      <th>KS</th>\n",
       "      <th>Minutes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>36</td>\n",
       "      <td>56497</td>\n",
       "      <td>40</td>\n",
       "      <td>4929</td>\n",
       "      <td>0.919206</td>\n",
       "      <td>0.007251</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.014283</td>\n",
       "      <td>0.741644</td>\n",
       "      <td>0.483288</td>\n",
       "      <td>0.361981</td>\n",
       "      <td>0.131119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>LogisticRegression RFE</td>\n",
       "      <td>37</td>\n",
       "      <td>56499</td>\n",
       "      <td>38</td>\n",
       "      <td>4928</td>\n",
       "      <td>0.919255</td>\n",
       "      <td>0.007452</td>\n",
       "      <td>0.493333</td>\n",
       "      <td>0.014683</td>\n",
       "      <td>0.742399</td>\n",
       "      <td>0.484797</td>\n",
       "      <td>0.365048</td>\n",
       "      <td>41.746602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>LogisticRegression RFECV</td>\n",
       "      <td>0</td>\n",
       "      <td>56537</td>\n",
       "      <td>0</td>\n",
       "      <td>4965</td>\n",
       "      <td>0.919271</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.539789</td>\n",
       "      <td>0.079578</td>\n",
       "      <td>0.093007</td>\n",
       "      <td>189.196640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>LogisticRegression SM</td>\n",
       "      <td>3226</td>\n",
       "      <td>39654</td>\n",
       "      <td>16883</td>\n",
       "      <td>1739</td>\n",
       "      <td>0.697213</td>\n",
       "      <td>0.649748</td>\n",
       "      <td>0.160426</td>\n",
       "      <td>0.257318</td>\n",
       "      <td>0.735386</td>\n",
       "      <td>0.470772</td>\n",
       "      <td>0.347739</td>\n",
       "      <td>3.915135</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            TP     TN     FP    FN  Accuracy    Recall  \\\n",
       "LogisticRegression          36  56497     40  4929  0.919206  0.007251   \n",
       "LogisticRegression RFE      37  56499     38  4928  0.919255  0.007452   \n",
       "LogisticRegression RFECV     0  56537      0  4965  0.919271  0.000000   \n",
       "LogisticRegression SM     3226  39654  16883  1739  0.697213  0.649748   \n",
       "\n",
       "                          Precision        F1       AUC      Gini        KS  \\\n",
       "LogisticRegression         0.473684  0.014283  0.741644  0.483288  0.361981   \n",
       "LogisticRegression RFE     0.493333  0.014683  0.742399  0.484797  0.365048   \n",
       "LogisticRegression RFECV   0.000000  0.000000  0.539789  0.079578  0.093007   \n",
       "LogisticRegression SM      0.160426  0.257318  0.735386  0.470772  0.347739   \n",
       "\n",
       "                             Minutes  \n",
       "LogisticRegression          0.131119  \n",
       "LogisticRegression RFE     41.746602  \n",
       "LogisticRegression RFECV  189.196640  \n",
       "LogisticRegression SM       3.915135  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bin</th>\n",
       "      <th>score_min</th>\n",
       "      <th>score_max</th>\n",
       "      <th>bads</th>\n",
       "      <th>total</th>\n",
       "      <th>goods</th>\n",
       "      <th>bad_rate</th>\n",
       "      <th>bads_cum</th>\n",
       "      <th>goods_cum</th>\n",
       "      <th>total_cum</th>\n",
       "      <th>bads_cum_rate</th>\n",
       "      <th>goods_cum_rate</th>\n",
       "      <th>total_cum_rate</th>\n",
       "      <th>KS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>(-0.001, 7687.625]</td>\n",
       "      <td>0.700416</td>\n",
       "      <td>0.975826</td>\n",
       "      <td>1734</td>\n",
       "      <td>7688</td>\n",
       "      <td>5954</td>\n",
       "      <td>0.225546</td>\n",
       "      <td>1734</td>\n",
       "      <td>5954</td>\n",
       "      <td>7688</td>\n",
       "      <td>0.349245</td>\n",
       "      <td>0.105312</td>\n",
       "      <td>0.125004</td>\n",
       "      <td>0.243933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>(7687.625, 15375.25]</td>\n",
       "      <td>0.568157</td>\n",
       "      <td>0.700416</td>\n",
       "      <td>1046</td>\n",
       "      <td>7688</td>\n",
       "      <td>6642</td>\n",
       "      <td>0.136056</td>\n",
       "      <td>2780</td>\n",
       "      <td>12596</td>\n",
       "      <td>15376</td>\n",
       "      <td>0.559919</td>\n",
       "      <td>0.222792</td>\n",
       "      <td>0.250008</td>\n",
       "      <td>0.337127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>(15375.25, 23062.875]</td>\n",
       "      <td>0.462324</td>\n",
       "      <td>0.568145</td>\n",
       "      <td>669</td>\n",
       "      <td>7687</td>\n",
       "      <td>7018</td>\n",
       "      <td>0.087030</td>\n",
       "      <td>3449</td>\n",
       "      <td>19614</td>\n",
       "      <td>23063</td>\n",
       "      <td>0.694663</td>\n",
       "      <td>0.346923</td>\n",
       "      <td>0.374996</td>\n",
       "      <td>0.347739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>(23062.875, 30750.5]</td>\n",
       "      <td>0.372843</td>\n",
       "      <td>0.462319</td>\n",
       "      <td>526</td>\n",
       "      <td>7688</td>\n",
       "      <td>7162</td>\n",
       "      <td>0.068418</td>\n",
       "      <td>3975</td>\n",
       "      <td>26776</td>\n",
       "      <td>30751</td>\n",
       "      <td>0.800604</td>\n",
       "      <td>0.473601</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.327003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>(30750.5, 38438.125]</td>\n",
       "      <td>0.294713</td>\n",
       "      <td>0.372820</td>\n",
       "      <td>361</td>\n",
       "      <td>7688</td>\n",
       "      <td>7327</td>\n",
       "      <td>0.046956</td>\n",
       "      <td>4336</td>\n",
       "      <td>34103</td>\n",
       "      <td>38439</td>\n",
       "      <td>0.873313</td>\n",
       "      <td>0.603198</td>\n",
       "      <td>0.625004</td>\n",
       "      <td>0.270115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>(38438.125, 46125.75]</td>\n",
       "      <td>0.222318</td>\n",
       "      <td>0.294694</td>\n",
       "      <td>286</td>\n",
       "      <td>7687</td>\n",
       "      <td>7401</td>\n",
       "      <td>0.037206</td>\n",
       "      <td>4622</td>\n",
       "      <td>41504</td>\n",
       "      <td>46126</td>\n",
       "      <td>0.930916</td>\n",
       "      <td>0.734103</td>\n",
       "      <td>0.749992</td>\n",
       "      <td>0.196813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>(46125.75, 53813.375]</td>\n",
       "      <td>0.147586</td>\n",
       "      <td>0.222304</td>\n",
       "      <td>214</td>\n",
       "      <td>7688</td>\n",
       "      <td>7474</td>\n",
       "      <td>0.027836</td>\n",
       "      <td>4836</td>\n",
       "      <td>48978</td>\n",
       "      <td>53814</td>\n",
       "      <td>0.974018</td>\n",
       "      <td>0.866300</td>\n",
       "      <td>0.874996</td>\n",
       "      <td>0.107718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>(53813.375, 61501.0]</td>\n",
       "      <td>0.004996</td>\n",
       "      <td>0.147584</td>\n",
       "      <td>129</td>\n",
       "      <td>7688</td>\n",
       "      <td>7559</td>\n",
       "      <td>0.016779</td>\n",
       "      <td>4965</td>\n",
       "      <td>56537</td>\n",
       "      <td>61502</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     bin  score_min  score_max  bads  total  goods  bad_rate  \\\n",
       "0     (-0.001, 7687.625]   0.700416   0.975826  1734   7688   5954  0.225546   \n",
       "1   (7687.625, 15375.25]   0.568157   0.700416  1046   7688   6642  0.136056   \n",
       "2  (15375.25, 23062.875]   0.462324   0.568145   669   7687   7018  0.087030   \n",
       "3   (23062.875, 30750.5]   0.372843   0.462319   526   7688   7162  0.068418   \n",
       "4   (30750.5, 38438.125]   0.294713   0.372820   361   7688   7327  0.046956   \n",
       "5  (38438.125, 46125.75]   0.222318   0.294694   286   7687   7401  0.037206   \n",
       "6  (46125.75, 53813.375]   0.147586   0.222304   214   7688   7474  0.027836   \n",
       "7   (53813.375, 61501.0]   0.004996   0.147584   129   7688   7559  0.016779   \n",
       "\n",
       "   bads_cum  goods_cum  total_cum  bads_cum_rate  goods_cum_rate  \\\n",
       "0      1734       5954       7688       0.349245        0.105312   \n",
       "1      2780      12596      15376       0.559919        0.222792   \n",
       "2      3449      19614      23063       0.694663        0.346923   \n",
       "3      3975      26776      30751       0.800604        0.473601   \n",
       "4      4336      34103      38439       0.873313        0.603198   \n",
       "5      4622      41504      46126       0.930916        0.734103   \n",
       "6      4836      48978      53814       0.974018        0.866300   \n",
       "7      4965      56537      61502       1.000000        1.000000   \n",
       "\n",
       "   total_cum_rate        KS  \n",
       "0        0.125004  0.243933  \n",
       "1        0.250008  0.337127  \n",
       "2        0.374996  0.347739  \n",
       "3        0.500000  0.327003  \n",
       "4        0.625004  0.270115  \n",
       "5        0.749992  0.196813  \n",
       "6        0.874996  0.107718  \n",
       "7        1.000000  0.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "X_train_2_sm, y_train_2_sm = smote.fit_sample(X_train_2, y_train_2)\n",
    "\n",
    "model_pipeline_LR_SM.fit(X_train_2_sm,y_train_2_sm)\n",
    "y_pred = model_pipeline_LR_SM.predict(X_test_2)\n",
    "y_pred_proba = model_pipeline_LR_SM.predict_proba(X_test_2)\n",
    "\n",
    "end = time.time()\n",
    "time_model=end-start\n",
    "time_model=time_model/60\n",
    "\n",
    "score_LR_SM, df_LR_SM = modelmetrics('LogisticRegression SM', y_pred, y_pred_proba, y_test_2,time_model) \n",
    "\n",
    "model_compare= pd.concat([model_compare,df_LR_SM])\n",
    "model_compare_LR_SM = model_compare\n",
    "\n",
    "display(model_compare, score_LR_SM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Naive Bayes (NO FS NO SM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pipeline_NB = make_pipeline(MultinomialNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TP</th>\n",
       "      <th>TN</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Gini</th>\n",
       "      <th>KS</th>\n",
       "      <th>Minutes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>36</td>\n",
       "      <td>56497</td>\n",
       "      <td>40</td>\n",
       "      <td>4929</td>\n",
       "      <td>0.919206</td>\n",
       "      <td>0.007251</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.014283</td>\n",
       "      <td>0.741644</td>\n",
       "      <td>0.483288</td>\n",
       "      <td>0.361981</td>\n",
       "      <td>0.131119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>LogisticRegression RFE</td>\n",
       "      <td>37</td>\n",
       "      <td>56499</td>\n",
       "      <td>38</td>\n",
       "      <td>4928</td>\n",
       "      <td>0.919255</td>\n",
       "      <td>0.007452</td>\n",
       "      <td>0.493333</td>\n",
       "      <td>0.014683</td>\n",
       "      <td>0.742399</td>\n",
       "      <td>0.484797</td>\n",
       "      <td>0.365048</td>\n",
       "      <td>41.746602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>LogisticRegression RFECV</td>\n",
       "      <td>0</td>\n",
       "      <td>56537</td>\n",
       "      <td>0</td>\n",
       "      <td>4965</td>\n",
       "      <td>0.919271</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.539789</td>\n",
       "      <td>0.079578</td>\n",
       "      <td>0.093007</td>\n",
       "      <td>189.196640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>LogisticRegression SM</td>\n",
       "      <td>3226</td>\n",
       "      <td>39654</td>\n",
       "      <td>16883</td>\n",
       "      <td>1739</td>\n",
       "      <td>0.697213</td>\n",
       "      <td>0.649748</td>\n",
       "      <td>0.160426</td>\n",
       "      <td>0.257318</td>\n",
       "      <td>0.735386</td>\n",
       "      <td>0.470772</td>\n",
       "      <td>0.347739</td>\n",
       "      <td>3.915135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NaiveBayes</td>\n",
       "      <td>106</td>\n",
       "      <td>56141</td>\n",
       "      <td>396</td>\n",
       "      <td>4859</td>\n",
       "      <td>0.914556</td>\n",
       "      <td>0.021349</td>\n",
       "      <td>0.211155</td>\n",
       "      <td>0.038778</td>\n",
       "      <td>0.631575</td>\n",
       "      <td>0.263150</td>\n",
       "      <td>0.192399</td>\n",
       "      <td>0.004301</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            TP     TN     FP    FN  Accuracy    Recall  \\\n",
       "LogisticRegression          36  56497     40  4929  0.919206  0.007251   \n",
       "LogisticRegression RFE      37  56499     38  4928  0.919255  0.007452   \n",
       "LogisticRegression RFECV     0  56537      0  4965  0.919271  0.000000   \n",
       "LogisticRegression SM     3226  39654  16883  1739  0.697213  0.649748   \n",
       "NaiveBayes                 106  56141    396  4859  0.914556  0.021349   \n",
       "\n",
       "                          Precision        F1       AUC      Gini        KS  \\\n",
       "LogisticRegression         0.473684  0.014283  0.741644  0.483288  0.361981   \n",
       "LogisticRegression RFE     0.493333  0.014683  0.742399  0.484797  0.365048   \n",
       "LogisticRegression RFECV   0.000000  0.000000  0.539789  0.079578  0.093007   \n",
       "LogisticRegression SM      0.160426  0.257318  0.735386  0.470772  0.347739   \n",
       "NaiveBayes                 0.211155  0.038778  0.631575  0.263150  0.192399   \n",
       "\n",
       "                             Minutes  \n",
       "LogisticRegression          0.131119  \n",
       "LogisticRegression RFE     41.746602  \n",
       "LogisticRegression RFECV  189.196640  \n",
       "LogisticRegression SM       3.915135  \n",
       "NaiveBayes                  0.004301  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bin</th>\n",
       "      <th>score_min</th>\n",
       "      <th>score_max</th>\n",
       "      <th>bads</th>\n",
       "      <th>total</th>\n",
       "      <th>goods</th>\n",
       "      <th>bad_rate</th>\n",
       "      <th>bads_cum</th>\n",
       "      <th>goods_cum</th>\n",
       "      <th>total_cum</th>\n",
       "      <th>bads_cum_rate</th>\n",
       "      <th>goods_cum_rate</th>\n",
       "      <th>total_cum_rate</th>\n",
       "      <th>KS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>(-0.001, 7687.625]</td>\n",
       "      <td>0.232875</td>\n",
       "      <td>0.755576</td>\n",
       "      <td>1165</td>\n",
       "      <td>7688</td>\n",
       "      <td>6523</td>\n",
       "      <td>0.151535</td>\n",
       "      <td>1165</td>\n",
       "      <td>6523</td>\n",
       "      <td>7688</td>\n",
       "      <td>0.234642</td>\n",
       "      <td>0.115376</td>\n",
       "      <td>0.125004</td>\n",
       "      <td>0.119267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>(7687.625, 15375.25]</td>\n",
       "      <td>0.147297</td>\n",
       "      <td>0.232863</td>\n",
       "      <td>846</td>\n",
       "      <td>7688</td>\n",
       "      <td>6842</td>\n",
       "      <td>0.110042</td>\n",
       "      <td>2011</td>\n",
       "      <td>13365</td>\n",
       "      <td>15376</td>\n",
       "      <td>0.405035</td>\n",
       "      <td>0.236394</td>\n",
       "      <td>0.250008</td>\n",
       "      <td>0.168641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>(15375.25, 23062.875]</td>\n",
       "      <td>0.097051</td>\n",
       "      <td>0.147285</td>\n",
       "      <td>729</td>\n",
       "      <td>7687</td>\n",
       "      <td>6958</td>\n",
       "      <td>0.094835</td>\n",
       "      <td>2740</td>\n",
       "      <td>20323</td>\n",
       "      <td>23063</td>\n",
       "      <td>0.551863</td>\n",
       "      <td>0.359464</td>\n",
       "      <td>0.374996</td>\n",
       "      <td>0.192399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>(23062.875, 30750.5]</td>\n",
       "      <td>0.064643</td>\n",
       "      <td>0.097038</td>\n",
       "      <td>600</td>\n",
       "      <td>7688</td>\n",
       "      <td>7088</td>\n",
       "      <td>0.078044</td>\n",
       "      <td>3340</td>\n",
       "      <td>27411</td>\n",
       "      <td>30751</td>\n",
       "      <td>0.672709</td>\n",
       "      <td>0.484833</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.187876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>(30750.5, 38438.125]</td>\n",
       "      <td>0.041833</td>\n",
       "      <td>0.064643</td>\n",
       "      <td>529</td>\n",
       "      <td>7688</td>\n",
       "      <td>7159</td>\n",
       "      <td>0.068809</td>\n",
       "      <td>3869</td>\n",
       "      <td>34570</td>\n",
       "      <td>38439</td>\n",
       "      <td>0.779255</td>\n",
       "      <td>0.611458</td>\n",
       "      <td>0.625004</td>\n",
       "      <td>0.167797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>(38438.125, 46125.75]</td>\n",
       "      <td>0.026662</td>\n",
       "      <td>0.041829</td>\n",
       "      <td>436</td>\n",
       "      <td>7687</td>\n",
       "      <td>7251</td>\n",
       "      <td>0.056719</td>\n",
       "      <td>4305</td>\n",
       "      <td>41821</td>\n",
       "      <td>46126</td>\n",
       "      <td>0.867069</td>\n",
       "      <td>0.739710</td>\n",
       "      <td>0.749992</td>\n",
       "      <td>0.127359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>(46125.75, 53813.375]</td>\n",
       "      <td>0.015042</td>\n",
       "      <td>0.026661</td>\n",
       "      <td>364</td>\n",
       "      <td>7688</td>\n",
       "      <td>7324</td>\n",
       "      <td>0.047347</td>\n",
       "      <td>4669</td>\n",
       "      <td>49145</td>\n",
       "      <td>53814</td>\n",
       "      <td>0.940383</td>\n",
       "      <td>0.869254</td>\n",
       "      <td>0.874996</td>\n",
       "      <td>0.071129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>(53813.375, 61501.0]</td>\n",
       "      <td>0.000910</td>\n",
       "      <td>0.015042</td>\n",
       "      <td>296</td>\n",
       "      <td>7688</td>\n",
       "      <td>7392</td>\n",
       "      <td>0.038502</td>\n",
       "      <td>4965</td>\n",
       "      <td>56537</td>\n",
       "      <td>61502</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     bin  score_min  score_max  bads  total  goods  bad_rate  \\\n",
       "0     (-0.001, 7687.625]   0.232875   0.755576  1165   7688   6523  0.151535   \n",
       "1   (7687.625, 15375.25]   0.147297   0.232863   846   7688   6842  0.110042   \n",
       "2  (15375.25, 23062.875]   0.097051   0.147285   729   7687   6958  0.094835   \n",
       "3   (23062.875, 30750.5]   0.064643   0.097038   600   7688   7088  0.078044   \n",
       "4   (30750.5, 38438.125]   0.041833   0.064643   529   7688   7159  0.068809   \n",
       "5  (38438.125, 46125.75]   0.026662   0.041829   436   7687   7251  0.056719   \n",
       "6  (46125.75, 53813.375]   0.015042   0.026661   364   7688   7324  0.047347   \n",
       "7   (53813.375, 61501.0]   0.000910   0.015042   296   7688   7392  0.038502   \n",
       "\n",
       "   bads_cum  goods_cum  total_cum  bads_cum_rate  goods_cum_rate  \\\n",
       "0      1165       6523       7688       0.234642        0.115376   \n",
       "1      2011      13365      15376       0.405035        0.236394   \n",
       "2      2740      20323      23063       0.551863        0.359464   \n",
       "3      3340      27411      30751       0.672709        0.484833   \n",
       "4      3869      34570      38439       0.779255        0.611458   \n",
       "5      4305      41821      46126       0.867069        0.739710   \n",
       "6      4669      49145      53814       0.940383        0.869254   \n",
       "7      4965      56537      61502       1.000000        1.000000   \n",
       "\n",
       "   total_cum_rate        KS  \n",
       "0        0.125004  0.119267  \n",
       "1        0.250008  0.168641  \n",
       "2        0.374996  0.192399  \n",
       "3        0.500000  0.187876  \n",
       "4        0.625004  0.167797  \n",
       "5        0.749992  0.127359  \n",
       "6        0.874996  0.071129  \n",
       "7        1.000000  0.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "model_pipeline_NB.fit(X_train_2,y_train_2)\n",
    "y_pred = model_pipeline_NB.predict(X_test_2)\n",
    "y_pred_proba = model_pipeline_NB.predict_proba(X_test_2)\n",
    "\n",
    "end = time.time()\n",
    "time_model=end-start\n",
    "time_model=time_model/60\n",
    "\n",
    "score_NB, df_NB = modelmetrics('NaiveBayes', y_pred, y_pred_proba, y_test_2,time_model) \n",
    "\n",
    "model_compare= pd.concat([model_compare,df_NB])\n",
    "model_compare_NB = model_compare\n",
    "\n",
    "display(model_compare, score_NB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Naive Bayes Feature Selection RFECV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### scoring='neg_mean_squared_error'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf=KFold(n_splits=3, shuffle=True, random_state=1234)\n",
    "feature_selection_NB_RFECV_pipeline = make_pipeline(FeatureSelectionExecuter\n",
    "                                                    (RFECV(MultinomialNB(),\n",
    "                                                    cv=kf,scoring='neg_mean_squared_error')\n",
    "                                                     ,\"Yes\"))\n",
    "model_pipeline_NB_RFECV = make_pipeline(MultinomialNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TP</th>\n",
       "      <th>TN</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Gini</th>\n",
       "      <th>KS</th>\n",
       "      <th>Minutes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>36</td>\n",
       "      <td>56497</td>\n",
       "      <td>40</td>\n",
       "      <td>4929</td>\n",
       "      <td>0.919206</td>\n",
       "      <td>0.007251</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.014283</td>\n",
       "      <td>0.741644</td>\n",
       "      <td>0.483288</td>\n",
       "      <td>0.361981</td>\n",
       "      <td>0.131119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>LogisticRegression RFE</td>\n",
       "      <td>37</td>\n",
       "      <td>56499</td>\n",
       "      <td>38</td>\n",
       "      <td>4928</td>\n",
       "      <td>0.919255</td>\n",
       "      <td>0.007452</td>\n",
       "      <td>0.493333</td>\n",
       "      <td>0.014683</td>\n",
       "      <td>0.742399</td>\n",
       "      <td>0.484797</td>\n",
       "      <td>0.365048</td>\n",
       "      <td>41.746602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>LogisticRegression RFECV</td>\n",
       "      <td>0</td>\n",
       "      <td>56537</td>\n",
       "      <td>0</td>\n",
       "      <td>4965</td>\n",
       "      <td>0.919271</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.539789</td>\n",
       "      <td>0.079578</td>\n",
       "      <td>0.093007</td>\n",
       "      <td>189.196640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>LogisticRegression SM</td>\n",
       "      <td>3226</td>\n",
       "      <td>39654</td>\n",
       "      <td>16883</td>\n",
       "      <td>1739</td>\n",
       "      <td>0.697213</td>\n",
       "      <td>0.649748</td>\n",
       "      <td>0.160426</td>\n",
       "      <td>0.257318</td>\n",
       "      <td>0.735386</td>\n",
       "      <td>0.470772</td>\n",
       "      <td>0.347739</td>\n",
       "      <td>3.915135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NaiveBayes</td>\n",
       "      <td>106</td>\n",
       "      <td>56141</td>\n",
       "      <td>396</td>\n",
       "      <td>4859</td>\n",
       "      <td>0.914556</td>\n",
       "      <td>0.021349</td>\n",
       "      <td>0.211155</td>\n",
       "      <td>0.038778</td>\n",
       "      <td>0.631575</td>\n",
       "      <td>0.263150</td>\n",
       "      <td>0.192399</td>\n",
       "      <td>0.004301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NaiveBayes RFECV SqError</td>\n",
       "      <td>0</td>\n",
       "      <td>56537</td>\n",
       "      <td>0</td>\n",
       "      <td>4965</td>\n",
       "      <td>0.919271</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008951</td>\n",
       "      <td>4.736567</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            TP     TN     FP    FN  Accuracy    Recall  \\\n",
       "LogisticRegression          36  56497     40  4929  0.919206  0.007251   \n",
       "LogisticRegression RFE      37  56499     38  4928  0.919255  0.007452   \n",
       "LogisticRegression RFECV     0  56537      0  4965  0.919271  0.000000   \n",
       "LogisticRegression SM     3226  39654  16883  1739  0.697213  0.649748   \n",
       "NaiveBayes                 106  56141    396  4859  0.914556  0.021349   \n",
       "NaiveBayes RFECV SqError     0  56537      0  4965  0.919271  0.000000   \n",
       "\n",
       "                          Precision        F1       AUC      Gini        KS  \\\n",
       "LogisticRegression         0.473684  0.014283  0.741644  0.483288  0.361981   \n",
       "LogisticRegression RFE     0.493333  0.014683  0.742399  0.484797  0.365048   \n",
       "LogisticRegression RFECV   0.000000  0.000000  0.539789  0.079578  0.093007   \n",
       "LogisticRegression SM      0.160426  0.257318  0.735386  0.470772  0.347739   \n",
       "NaiveBayes                 0.211155  0.038778  0.631575  0.263150  0.192399   \n",
       "NaiveBayes RFECV SqError   0.000000  0.000000  0.500000  0.000000  0.008951   \n",
       "\n",
       "                             Minutes  \n",
       "LogisticRegression          0.131119  \n",
       "LogisticRegression RFE     41.746602  \n",
       "LogisticRegression RFECV  189.196640  \n",
       "LogisticRegression SM       3.915135  \n",
       "NaiveBayes                  0.004301  \n",
       "NaiveBayes RFECV SqError    4.736567  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bin</th>\n",
       "      <th>score_min</th>\n",
       "      <th>score_max</th>\n",
       "      <th>bads</th>\n",
       "      <th>total</th>\n",
       "      <th>goods</th>\n",
       "      <th>bad_rate</th>\n",
       "      <th>bads_cum</th>\n",
       "      <th>goods_cum</th>\n",
       "      <th>total_cum</th>\n",
       "      <th>bads_cum_rate</th>\n",
       "      <th>goods_cum_rate</th>\n",
       "      <th>total_cum_rate</th>\n",
       "      <th>KS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>(-0.001, 7687.625]</td>\n",
       "      <td>0.080725</td>\n",
       "      <td>0.080725</td>\n",
       "      <td>650</td>\n",
       "      <td>7688</td>\n",
       "      <td>7038</td>\n",
       "      <td>0.084547</td>\n",
       "      <td>650</td>\n",
       "      <td>7038</td>\n",
       "      <td>7688</td>\n",
       "      <td>0.130916</td>\n",
       "      <td>0.124485</td>\n",
       "      <td>0.125004</td>\n",
       "      <td>0.006432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>(7687.625, 15375.25]</td>\n",
       "      <td>0.080725</td>\n",
       "      <td>0.080725</td>\n",
       "      <td>614</td>\n",
       "      <td>7688</td>\n",
       "      <td>7074</td>\n",
       "      <td>0.079865</td>\n",
       "      <td>1264</td>\n",
       "      <td>14112</td>\n",
       "      <td>15376</td>\n",
       "      <td>0.254582</td>\n",
       "      <td>0.249606</td>\n",
       "      <td>0.250008</td>\n",
       "      <td>0.004976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>(15375.25, 23062.875]</td>\n",
       "      <td>0.080725</td>\n",
       "      <td>0.080725</td>\n",
       "      <td>621</td>\n",
       "      <td>7687</td>\n",
       "      <td>7066</td>\n",
       "      <td>0.080786</td>\n",
       "      <td>1885</td>\n",
       "      <td>21178</td>\n",
       "      <td>23063</td>\n",
       "      <td>0.379658</td>\n",
       "      <td>0.374587</td>\n",
       "      <td>0.374996</td>\n",
       "      <td>0.005071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>(23062.875, 30750.5]</td>\n",
       "      <td>0.080725</td>\n",
       "      <td>0.080725</td>\n",
       "      <td>634</td>\n",
       "      <td>7688</td>\n",
       "      <td>7054</td>\n",
       "      <td>0.082466</td>\n",
       "      <td>2519</td>\n",
       "      <td>28232</td>\n",
       "      <td>30751</td>\n",
       "      <td>0.507351</td>\n",
       "      <td>0.499354</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.007997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>(30750.5, 38438.125]</td>\n",
       "      <td>0.080725</td>\n",
       "      <td>0.080725</td>\n",
       "      <td>625</td>\n",
       "      <td>7688</td>\n",
       "      <td>7063</td>\n",
       "      <td>0.081296</td>\n",
       "      <td>3144</td>\n",
       "      <td>35295</td>\n",
       "      <td>38439</td>\n",
       "      <td>0.633233</td>\n",
       "      <td>0.624281</td>\n",
       "      <td>0.625004</td>\n",
       "      <td>0.008951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>(38438.125, 46125.75]</td>\n",
       "      <td>0.080725</td>\n",
       "      <td>0.080725</td>\n",
       "      <td>620</td>\n",
       "      <td>7687</td>\n",
       "      <td>7067</td>\n",
       "      <td>0.080656</td>\n",
       "      <td>3764</td>\n",
       "      <td>42362</td>\n",
       "      <td>46126</td>\n",
       "      <td>0.758107</td>\n",
       "      <td>0.749279</td>\n",
       "      <td>0.749992</td>\n",
       "      <td>0.008828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>(46125.75, 53813.375]</td>\n",
       "      <td>0.080725</td>\n",
       "      <td>0.080725</td>\n",
       "      <td>599</td>\n",
       "      <td>7688</td>\n",
       "      <td>7089</td>\n",
       "      <td>0.077914</td>\n",
       "      <td>4363</td>\n",
       "      <td>49451</td>\n",
       "      <td>53814</td>\n",
       "      <td>0.878751</td>\n",
       "      <td>0.874666</td>\n",
       "      <td>0.874996</td>\n",
       "      <td>0.004085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>(53813.375, 61501.0]</td>\n",
       "      <td>0.080725</td>\n",
       "      <td>0.080725</td>\n",
       "      <td>602</td>\n",
       "      <td>7688</td>\n",
       "      <td>7086</td>\n",
       "      <td>0.078304</td>\n",
       "      <td>4965</td>\n",
       "      <td>56537</td>\n",
       "      <td>61502</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     bin  score_min  score_max  bads  total  goods  bad_rate  \\\n",
       "0     (-0.001, 7687.625]   0.080725   0.080725   650   7688   7038  0.084547   \n",
       "1   (7687.625, 15375.25]   0.080725   0.080725   614   7688   7074  0.079865   \n",
       "2  (15375.25, 23062.875]   0.080725   0.080725   621   7687   7066  0.080786   \n",
       "3   (23062.875, 30750.5]   0.080725   0.080725   634   7688   7054  0.082466   \n",
       "4   (30750.5, 38438.125]   0.080725   0.080725   625   7688   7063  0.081296   \n",
       "5  (38438.125, 46125.75]   0.080725   0.080725   620   7687   7067  0.080656   \n",
       "6  (46125.75, 53813.375]   0.080725   0.080725   599   7688   7089  0.077914   \n",
       "7   (53813.375, 61501.0]   0.080725   0.080725   602   7688   7086  0.078304   \n",
       "\n",
       "   bads_cum  goods_cum  total_cum  bads_cum_rate  goods_cum_rate  \\\n",
       "0       650       7038       7688       0.130916        0.124485   \n",
       "1      1264      14112      15376       0.254582        0.249606   \n",
       "2      1885      21178      23063       0.379658        0.374587   \n",
       "3      2519      28232      30751       0.507351        0.499354   \n",
       "4      3144      35295      38439       0.633233        0.624281   \n",
       "5      3764      42362      46126       0.758107        0.749279   \n",
       "6      4363      49451      53814       0.878751        0.874666   \n",
       "7      4965      56537      61502       1.000000        1.000000   \n",
       "\n",
       "   total_cum_rate        KS  \n",
       "0        0.125004  0.006432  \n",
       "1        0.250008  0.004976  \n",
       "2        0.374996  0.005071  \n",
       "3        0.500000  0.007997  \n",
       "4        0.625004  0.008951  \n",
       "5        0.749992  0.008828  \n",
       "6        0.874996  0.004085  \n",
       "7        1.000000  0.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "X_train_2_fs_NB = feature_selection_NB_RFECV_pipeline.fit_transform(X_train_2,y_train_2)\n",
    "X_test_2_fs_NB = feature_selection_NB_RFECV_pipeline.transform(X_test_2)\n",
    "\n",
    "\n",
    "model_pipeline_NB_RFECV.fit(X_train_2_fs_NB,y_train_2)\n",
    "y_pred = model_pipeline_NB_RFECV.predict(X_test_2_fs_NB)\n",
    "y_pred_proba = model_pipeline_NB_RFECV.predict_proba(X_test_2_fs_NB)\n",
    "\n",
    "end = time.time()\n",
    "time_model=end-start\n",
    "time_model=time_model/60\n",
    "\n",
    "score_NB_RFECV, df_NB_RFECV = modelmetrics('NaiveBayes RFECV SqError', y_pred, y_pred_proba, y_test_2,time_model) \n",
    "\n",
    "model_compare=pd.concat([model_compare,df_NB_RFECV])\n",
    "model_compare_NB_RFECV = model_compare\n",
    "\n",
    "display(model_compare, score_NB_RFECV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### scoring='roc_auc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf=KFold(n_splits=3, shuffle=True, random_state=1234)\n",
    "feature_selection_NB_RFECV_pipeline_2 = make_pipeline(FeatureSelectionExecuter\n",
    "                                                    (RFECV(MultinomialNB(),\n",
    "                                                    cv=kf,scoring='roc_auc')\n",
    "                                                     ,\"Yes\"))\n",
    "model_pipeline_NB_RFECV_2 = make_pipeline(MultinomialNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TP</th>\n",
       "      <th>TN</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Gini</th>\n",
       "      <th>KS</th>\n",
       "      <th>Minutes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>36</td>\n",
       "      <td>56497</td>\n",
       "      <td>40</td>\n",
       "      <td>4929</td>\n",
       "      <td>0.919206</td>\n",
       "      <td>0.007251</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.014283</td>\n",
       "      <td>0.741644</td>\n",
       "      <td>0.483288</td>\n",
       "      <td>0.361981</td>\n",
       "      <td>0.131119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>LogisticRegression RFE</td>\n",
       "      <td>37</td>\n",
       "      <td>56499</td>\n",
       "      <td>38</td>\n",
       "      <td>4928</td>\n",
       "      <td>0.919255</td>\n",
       "      <td>0.007452</td>\n",
       "      <td>0.493333</td>\n",
       "      <td>0.014683</td>\n",
       "      <td>0.742399</td>\n",
       "      <td>0.484797</td>\n",
       "      <td>0.365048</td>\n",
       "      <td>41.746602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>LogisticRegression RFECV</td>\n",
       "      <td>0</td>\n",
       "      <td>56537</td>\n",
       "      <td>0</td>\n",
       "      <td>4965</td>\n",
       "      <td>0.919271</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.539789</td>\n",
       "      <td>0.079578</td>\n",
       "      <td>0.093007</td>\n",
       "      <td>189.196640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>LogisticRegression SM</td>\n",
       "      <td>3226</td>\n",
       "      <td>39654</td>\n",
       "      <td>16883</td>\n",
       "      <td>1739</td>\n",
       "      <td>0.697213</td>\n",
       "      <td>0.649748</td>\n",
       "      <td>0.160426</td>\n",
       "      <td>0.257318</td>\n",
       "      <td>0.735386</td>\n",
       "      <td>0.470772</td>\n",
       "      <td>0.347739</td>\n",
       "      <td>3.915135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NaiveBayes</td>\n",
       "      <td>106</td>\n",
       "      <td>56141</td>\n",
       "      <td>396</td>\n",
       "      <td>4859</td>\n",
       "      <td>0.914556</td>\n",
       "      <td>0.021349</td>\n",
       "      <td>0.211155</td>\n",
       "      <td>0.038778</td>\n",
       "      <td>0.631575</td>\n",
       "      <td>0.263150</td>\n",
       "      <td>0.192399</td>\n",
       "      <td>0.004301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NaiveBayes RFECV SqError</td>\n",
       "      <td>0</td>\n",
       "      <td>56537</td>\n",
       "      <td>0</td>\n",
       "      <td>4965</td>\n",
       "      <td>0.919271</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008951</td>\n",
       "      <td>4.736567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NaiveBayes RFECV AUC</td>\n",
       "      <td>97</td>\n",
       "      <td>56171</td>\n",
       "      <td>366</td>\n",
       "      <td>4868</td>\n",
       "      <td>0.914897</td>\n",
       "      <td>0.019537</td>\n",
       "      <td>0.209503</td>\n",
       "      <td>0.035741</td>\n",
       "      <td>0.631669</td>\n",
       "      <td>0.263338</td>\n",
       "      <td>0.192618</td>\n",
       "      <td>4.469134</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            TP     TN     FP    FN  Accuracy    Recall  \\\n",
       "LogisticRegression          36  56497     40  4929  0.919206  0.007251   \n",
       "LogisticRegression RFE      37  56499     38  4928  0.919255  0.007452   \n",
       "LogisticRegression RFECV     0  56537      0  4965  0.919271  0.000000   \n",
       "LogisticRegression SM     3226  39654  16883  1739  0.697213  0.649748   \n",
       "NaiveBayes                 106  56141    396  4859  0.914556  0.021349   \n",
       "NaiveBayes RFECV SqError     0  56537      0  4965  0.919271  0.000000   \n",
       "NaiveBayes RFECV AUC        97  56171    366  4868  0.914897  0.019537   \n",
       "\n",
       "                          Precision        F1       AUC      Gini        KS  \\\n",
       "LogisticRegression         0.473684  0.014283  0.741644  0.483288  0.361981   \n",
       "LogisticRegression RFE     0.493333  0.014683  0.742399  0.484797  0.365048   \n",
       "LogisticRegression RFECV   0.000000  0.000000  0.539789  0.079578  0.093007   \n",
       "LogisticRegression SM      0.160426  0.257318  0.735386  0.470772  0.347739   \n",
       "NaiveBayes                 0.211155  0.038778  0.631575  0.263150  0.192399   \n",
       "NaiveBayes RFECV SqError   0.000000  0.000000  0.500000  0.000000  0.008951   \n",
       "NaiveBayes RFECV AUC       0.209503  0.035741  0.631669  0.263338  0.192618   \n",
       "\n",
       "                             Minutes  \n",
       "LogisticRegression          0.131119  \n",
       "LogisticRegression RFE     41.746602  \n",
       "LogisticRegression RFECV  189.196640  \n",
       "LogisticRegression SM       3.915135  \n",
       "NaiveBayes                  0.004301  \n",
       "NaiveBayes RFECV SqError    4.736567  \n",
       "NaiveBayes RFECV AUC        4.469134  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bin</th>\n",
       "      <th>score_min</th>\n",
       "      <th>score_max</th>\n",
       "      <th>bads</th>\n",
       "      <th>total</th>\n",
       "      <th>goods</th>\n",
       "      <th>bad_rate</th>\n",
       "      <th>bads_cum</th>\n",
       "      <th>goods_cum</th>\n",
       "      <th>total_cum</th>\n",
       "      <th>bads_cum_rate</th>\n",
       "      <th>goods_cum_rate</th>\n",
       "      <th>total_cum_rate</th>\n",
       "      <th>KS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>(-0.001, 7687.625]</td>\n",
       "      <td>0.229537</td>\n",
       "      <td>0.749538</td>\n",
       "      <td>1162</td>\n",
       "      <td>7688</td>\n",
       "      <td>6526</td>\n",
       "      <td>0.151145</td>\n",
       "      <td>1162</td>\n",
       "      <td>6526</td>\n",
       "      <td>7688</td>\n",
       "      <td>0.234038</td>\n",
       "      <td>0.115429</td>\n",
       "      <td>0.125004</td>\n",
       "      <td>0.118609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>(7687.625, 15375.25]</td>\n",
       "      <td>0.144882</td>\n",
       "      <td>0.229535</td>\n",
       "      <td>841</td>\n",
       "      <td>7688</td>\n",
       "      <td>6847</td>\n",
       "      <td>0.109391</td>\n",
       "      <td>2003</td>\n",
       "      <td>13373</td>\n",
       "      <td>15376</td>\n",
       "      <td>0.403424</td>\n",
       "      <td>0.236535</td>\n",
       "      <td>0.250008</td>\n",
       "      <td>0.166889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>(15375.25, 23062.875]</td>\n",
       "      <td>0.095348</td>\n",
       "      <td>0.144874</td>\n",
       "      <td>738</td>\n",
       "      <td>7687</td>\n",
       "      <td>6949</td>\n",
       "      <td>0.096006</td>\n",
       "      <td>2741</td>\n",
       "      <td>20322</td>\n",
       "      <td>23063</td>\n",
       "      <td>0.552064</td>\n",
       "      <td>0.359446</td>\n",
       "      <td>0.374996</td>\n",
       "      <td>0.192618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>(23062.875, 30750.5]</td>\n",
       "      <td>0.063433</td>\n",
       "      <td>0.095335</td>\n",
       "      <td>599</td>\n",
       "      <td>7688</td>\n",
       "      <td>7089</td>\n",
       "      <td>0.077914</td>\n",
       "      <td>3340</td>\n",
       "      <td>27411</td>\n",
       "      <td>30751</td>\n",
       "      <td>0.672709</td>\n",
       "      <td>0.484833</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.187876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>(30750.5, 38438.125]</td>\n",
       "      <td>0.041741</td>\n",
       "      <td>0.063430</td>\n",
       "      <td>524</td>\n",
       "      <td>7688</td>\n",
       "      <td>7164</td>\n",
       "      <td>0.068158</td>\n",
       "      <td>3864</td>\n",
       "      <td>34575</td>\n",
       "      <td>38439</td>\n",
       "      <td>0.778248</td>\n",
       "      <td>0.611546</td>\n",
       "      <td>0.625004</td>\n",
       "      <td>0.166701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>(38438.125, 46125.75]</td>\n",
       "      <td>0.027432</td>\n",
       "      <td>0.041740</td>\n",
       "      <td>446</td>\n",
       "      <td>7687</td>\n",
       "      <td>7241</td>\n",
       "      <td>0.058020</td>\n",
       "      <td>4310</td>\n",
       "      <td>41816</td>\n",
       "      <td>46126</td>\n",
       "      <td>0.868077</td>\n",
       "      <td>0.739622</td>\n",
       "      <td>0.749992</td>\n",
       "      <td>0.128455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>(46125.75, 53813.375]</td>\n",
       "      <td>0.015729</td>\n",
       "      <td>0.027432</td>\n",
       "      <td>359</td>\n",
       "      <td>7688</td>\n",
       "      <td>7329</td>\n",
       "      <td>0.046696</td>\n",
       "      <td>4669</td>\n",
       "      <td>49145</td>\n",
       "      <td>53814</td>\n",
       "      <td>0.940383</td>\n",
       "      <td>0.869254</td>\n",
       "      <td>0.874996</td>\n",
       "      <td>0.071129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>(53813.375, 61501.0]</td>\n",
       "      <td>0.000943</td>\n",
       "      <td>0.015728</td>\n",
       "      <td>296</td>\n",
       "      <td>7688</td>\n",
       "      <td>7392</td>\n",
       "      <td>0.038502</td>\n",
       "      <td>4965</td>\n",
       "      <td>56537</td>\n",
       "      <td>61502</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     bin  score_min  score_max  bads  total  goods  bad_rate  \\\n",
       "0     (-0.001, 7687.625]   0.229537   0.749538  1162   7688   6526  0.151145   \n",
       "1   (7687.625, 15375.25]   0.144882   0.229535   841   7688   6847  0.109391   \n",
       "2  (15375.25, 23062.875]   0.095348   0.144874   738   7687   6949  0.096006   \n",
       "3   (23062.875, 30750.5]   0.063433   0.095335   599   7688   7089  0.077914   \n",
       "4   (30750.5, 38438.125]   0.041741   0.063430   524   7688   7164  0.068158   \n",
       "5  (38438.125, 46125.75]   0.027432   0.041740   446   7687   7241  0.058020   \n",
       "6  (46125.75, 53813.375]   0.015729   0.027432   359   7688   7329  0.046696   \n",
       "7   (53813.375, 61501.0]   0.000943   0.015728   296   7688   7392  0.038502   \n",
       "\n",
       "   bads_cum  goods_cum  total_cum  bads_cum_rate  goods_cum_rate  \\\n",
       "0      1162       6526       7688       0.234038        0.115429   \n",
       "1      2003      13373      15376       0.403424        0.236535   \n",
       "2      2741      20322      23063       0.552064        0.359446   \n",
       "3      3340      27411      30751       0.672709        0.484833   \n",
       "4      3864      34575      38439       0.778248        0.611546   \n",
       "5      4310      41816      46126       0.868077        0.739622   \n",
       "6      4669      49145      53814       0.940383        0.869254   \n",
       "7      4965      56537      61502       1.000000        1.000000   \n",
       "\n",
       "   total_cum_rate        KS  \n",
       "0        0.125004  0.118609  \n",
       "1        0.250008  0.166889  \n",
       "2        0.374996  0.192618  \n",
       "3        0.500000  0.187876  \n",
       "4        0.625004  0.166701  \n",
       "5        0.749992  0.128455  \n",
       "6        0.874996  0.071129  \n",
       "7        1.000000  0.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "X_train_2_fs_NB_2 = feature_selection_NB_RFECV_pipeline_2.fit_transform(X_train_2,y_train_2)\n",
    "X_test_2_fs_NB_2 = feature_selection_NB_RFECV_pipeline_2.transform(X_test_2)\n",
    "\n",
    "\n",
    "model_pipeline_NB_RFECV_2.fit(X_train_2_fs_NB_2,y_train_2)\n",
    "y_pred = model_pipeline_NB_RFECV_2.predict(X_test_2_fs_NB_2)\n",
    "y_pred_proba = model_pipeline_NB_RFECV_2.predict_proba(X_test_2_fs_NB_2)\n",
    "\n",
    "end = time.time()\n",
    "time_model=end-start\n",
    "time_model=time_model/60\n",
    "\n",
    "score_NB_RFECV_2, df_NB_RFECV_2 = modelmetrics('NaiveBayes RFECV AUC', y_pred, y_pred_proba, y_test_2,time_model) \n",
    "\n",
    "model_compare=pd.concat([model_compare,df_NB_RFECV_2])\n",
    "model_compare_NB_RFECV_2 = model_compare\n",
    "\n",
    "display(model_compare, score_NB_RFECV_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Naive Bayes SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#smote is not in a pipeline\n",
    "smote= SMOTE(random_state=101)\n",
    "model_pipeline_NB_SM = make_pipeline(MultinomialNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TP</th>\n",
       "      <th>TN</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Gini</th>\n",
       "      <th>KS</th>\n",
       "      <th>Minutes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>36</td>\n",
       "      <td>56497</td>\n",
       "      <td>40</td>\n",
       "      <td>4929</td>\n",
       "      <td>0.919206</td>\n",
       "      <td>0.007251</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.014283</td>\n",
       "      <td>0.741644</td>\n",
       "      <td>0.483288</td>\n",
       "      <td>0.361981</td>\n",
       "      <td>0.131119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>LogisticRegression RFE</td>\n",
       "      <td>37</td>\n",
       "      <td>56499</td>\n",
       "      <td>38</td>\n",
       "      <td>4928</td>\n",
       "      <td>0.919255</td>\n",
       "      <td>0.007452</td>\n",
       "      <td>0.493333</td>\n",
       "      <td>0.014683</td>\n",
       "      <td>0.742399</td>\n",
       "      <td>0.484797</td>\n",
       "      <td>0.365048</td>\n",
       "      <td>41.746602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>LogisticRegression RFECV</td>\n",
       "      <td>0</td>\n",
       "      <td>56537</td>\n",
       "      <td>0</td>\n",
       "      <td>4965</td>\n",
       "      <td>0.919271</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.539789</td>\n",
       "      <td>0.079578</td>\n",
       "      <td>0.093007</td>\n",
       "      <td>189.196640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>LogisticRegression SM</td>\n",
       "      <td>3226</td>\n",
       "      <td>39654</td>\n",
       "      <td>16883</td>\n",
       "      <td>1739</td>\n",
       "      <td>0.697213</td>\n",
       "      <td>0.649748</td>\n",
       "      <td>0.160426</td>\n",
       "      <td>0.257318</td>\n",
       "      <td>0.735386</td>\n",
       "      <td>0.470772</td>\n",
       "      <td>0.347739</td>\n",
       "      <td>3.915135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NaiveBayes</td>\n",
       "      <td>106</td>\n",
       "      <td>56141</td>\n",
       "      <td>396</td>\n",
       "      <td>4859</td>\n",
       "      <td>0.914556</td>\n",
       "      <td>0.021349</td>\n",
       "      <td>0.211155</td>\n",
       "      <td>0.038778</td>\n",
       "      <td>0.631575</td>\n",
       "      <td>0.263150</td>\n",
       "      <td>0.192399</td>\n",
       "      <td>0.004301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NaiveBayes RFECV SqError</td>\n",
       "      <td>0</td>\n",
       "      <td>56537</td>\n",
       "      <td>0</td>\n",
       "      <td>4965</td>\n",
       "      <td>0.919271</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008951</td>\n",
       "      <td>4.736567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NaiveBayes RFECV AUC</td>\n",
       "      <td>97</td>\n",
       "      <td>56171</td>\n",
       "      <td>366</td>\n",
       "      <td>4868</td>\n",
       "      <td>0.914897</td>\n",
       "      <td>0.019537</td>\n",
       "      <td>0.209503</td>\n",
       "      <td>0.035741</td>\n",
       "      <td>0.631669</td>\n",
       "      <td>0.263338</td>\n",
       "      <td>0.192618</td>\n",
       "      <td>4.469134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NaiveBayes SM</td>\n",
       "      <td>2952</td>\n",
       "      <td>33496</td>\n",
       "      <td>23041</td>\n",
       "      <td>2013</td>\n",
       "      <td>0.592631</td>\n",
       "      <td>0.594562</td>\n",
       "      <td>0.113569</td>\n",
       "      <td>0.190710</td>\n",
       "      <td>0.631469</td>\n",
       "      <td>0.262938</td>\n",
       "      <td>0.187360</td>\n",
       "      <td>1.694798</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            TP     TN     FP    FN  Accuracy    Recall  \\\n",
       "LogisticRegression          36  56497     40  4929  0.919206  0.007251   \n",
       "LogisticRegression RFE      37  56499     38  4928  0.919255  0.007452   \n",
       "LogisticRegression RFECV     0  56537      0  4965  0.919271  0.000000   \n",
       "LogisticRegression SM     3226  39654  16883  1739  0.697213  0.649748   \n",
       "NaiveBayes                 106  56141    396  4859  0.914556  0.021349   \n",
       "NaiveBayes RFECV SqError     0  56537      0  4965  0.919271  0.000000   \n",
       "NaiveBayes RFECV AUC        97  56171    366  4868  0.914897  0.019537   \n",
       "NaiveBayes SM             2952  33496  23041  2013  0.592631  0.594562   \n",
       "\n",
       "                          Precision        F1       AUC      Gini        KS  \\\n",
       "LogisticRegression         0.473684  0.014283  0.741644  0.483288  0.361981   \n",
       "LogisticRegression RFE     0.493333  0.014683  0.742399  0.484797  0.365048   \n",
       "LogisticRegression RFECV   0.000000  0.000000  0.539789  0.079578  0.093007   \n",
       "LogisticRegression SM      0.160426  0.257318  0.735386  0.470772  0.347739   \n",
       "NaiveBayes                 0.211155  0.038778  0.631575  0.263150  0.192399   \n",
       "NaiveBayes RFECV SqError   0.000000  0.000000  0.500000  0.000000  0.008951   \n",
       "NaiveBayes RFECV AUC       0.209503  0.035741  0.631669  0.263338  0.192618   \n",
       "NaiveBayes SM              0.113569  0.190710  0.631469  0.262938  0.187360   \n",
       "\n",
       "                             Minutes  \n",
       "LogisticRegression          0.131119  \n",
       "LogisticRegression RFE     41.746602  \n",
       "LogisticRegression RFECV  189.196640  \n",
       "LogisticRegression SM       3.915135  \n",
       "NaiveBayes                  0.004301  \n",
       "NaiveBayes RFECV SqError    4.736567  \n",
       "NaiveBayes RFECV AUC        4.469134  \n",
       "NaiveBayes SM               1.694798  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bin</th>\n",
       "      <th>score_min</th>\n",
       "      <th>score_max</th>\n",
       "      <th>bads</th>\n",
       "      <th>total</th>\n",
       "      <th>goods</th>\n",
       "      <th>bad_rate</th>\n",
       "      <th>bads_cum</th>\n",
       "      <th>goods_cum</th>\n",
       "      <th>total_cum</th>\n",
       "      <th>bads_cum_rate</th>\n",
       "      <th>goods_cum_rate</th>\n",
       "      <th>total_cum_rate</th>\n",
       "      <th>KS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>(-0.001, 7687.625]</td>\n",
       "      <td>0.778074</td>\n",
       "      <td>0.963686</td>\n",
       "      <td>1170</td>\n",
       "      <td>7688</td>\n",
       "      <td>6518</td>\n",
       "      <td>0.152185</td>\n",
       "      <td>1170</td>\n",
       "      <td>6518</td>\n",
       "      <td>7688</td>\n",
       "      <td>0.235650</td>\n",
       "      <td>0.115287</td>\n",
       "      <td>0.125004</td>\n",
       "      <td>0.120362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>(7687.625, 15375.25]</td>\n",
       "      <td>0.660266</td>\n",
       "      <td>0.778073</td>\n",
       "      <td>834</td>\n",
       "      <td>7688</td>\n",
       "      <td>6854</td>\n",
       "      <td>0.108481</td>\n",
       "      <td>2004</td>\n",
       "      <td>13372</td>\n",
       "      <td>15376</td>\n",
       "      <td>0.403625</td>\n",
       "      <td>0.236518</td>\n",
       "      <td>0.250008</td>\n",
       "      <td>0.167108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>(15375.25, 23062.875]</td>\n",
       "      <td>0.543583</td>\n",
       "      <td>0.660262</td>\n",
       "      <td>713</td>\n",
       "      <td>7687</td>\n",
       "      <td>6974</td>\n",
       "      <td>0.092754</td>\n",
       "      <td>2717</td>\n",
       "      <td>20346</td>\n",
       "      <td>23063</td>\n",
       "      <td>0.547231</td>\n",
       "      <td>0.359871</td>\n",
       "      <td>0.374996</td>\n",
       "      <td>0.187360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>(23062.875, 30750.5]</td>\n",
       "      <td>0.426721</td>\n",
       "      <td>0.543580</td>\n",
       "      <td>601</td>\n",
       "      <td>7688</td>\n",
       "      <td>7087</td>\n",
       "      <td>0.078174</td>\n",
       "      <td>3318</td>\n",
       "      <td>27433</td>\n",
       "      <td>30751</td>\n",
       "      <td>0.668278</td>\n",
       "      <td>0.485222</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.183056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>(30750.5, 38438.125]</td>\n",
       "      <td>0.318736</td>\n",
       "      <td>0.426706</td>\n",
       "      <td>558</td>\n",
       "      <td>7688</td>\n",
       "      <td>7130</td>\n",
       "      <td>0.072581</td>\n",
       "      <td>3876</td>\n",
       "      <td>34563</td>\n",
       "      <td>38439</td>\n",
       "      <td>0.780665</td>\n",
       "      <td>0.611334</td>\n",
       "      <td>0.625004</td>\n",
       "      <td>0.169330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>(38438.125, 46125.75]</td>\n",
       "      <td>0.223065</td>\n",
       "      <td>0.318726</td>\n",
       "      <td>435</td>\n",
       "      <td>7687</td>\n",
       "      <td>7252</td>\n",
       "      <td>0.056589</td>\n",
       "      <td>4311</td>\n",
       "      <td>41815</td>\n",
       "      <td>46126</td>\n",
       "      <td>0.868278</td>\n",
       "      <td>0.739604</td>\n",
       "      <td>0.749992</td>\n",
       "      <td>0.128674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>(46125.75, 53813.375]</td>\n",
       "      <td>0.131906</td>\n",
       "      <td>0.223059</td>\n",
       "      <td>372</td>\n",
       "      <td>7688</td>\n",
       "      <td>7316</td>\n",
       "      <td>0.048387</td>\n",
       "      <td>4683</td>\n",
       "      <td>49131</td>\n",
       "      <td>53814</td>\n",
       "      <td>0.943202</td>\n",
       "      <td>0.869006</td>\n",
       "      <td>0.874996</td>\n",
       "      <td>0.074196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>(53813.375, 61501.0]</td>\n",
       "      <td>0.003129</td>\n",
       "      <td>0.131895</td>\n",
       "      <td>282</td>\n",
       "      <td>7688</td>\n",
       "      <td>7406</td>\n",
       "      <td>0.036681</td>\n",
       "      <td>4965</td>\n",
       "      <td>56537</td>\n",
       "      <td>61502</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     bin  score_min  score_max  bads  total  goods  bad_rate  \\\n",
       "0     (-0.001, 7687.625]   0.778074   0.963686  1170   7688   6518  0.152185   \n",
       "1   (7687.625, 15375.25]   0.660266   0.778073   834   7688   6854  0.108481   \n",
       "2  (15375.25, 23062.875]   0.543583   0.660262   713   7687   6974  0.092754   \n",
       "3   (23062.875, 30750.5]   0.426721   0.543580   601   7688   7087  0.078174   \n",
       "4   (30750.5, 38438.125]   0.318736   0.426706   558   7688   7130  0.072581   \n",
       "5  (38438.125, 46125.75]   0.223065   0.318726   435   7687   7252  0.056589   \n",
       "6  (46125.75, 53813.375]   0.131906   0.223059   372   7688   7316  0.048387   \n",
       "7   (53813.375, 61501.0]   0.003129   0.131895   282   7688   7406  0.036681   \n",
       "\n",
       "   bads_cum  goods_cum  total_cum  bads_cum_rate  goods_cum_rate  \\\n",
       "0      1170       6518       7688       0.235650        0.115287   \n",
       "1      2004      13372      15376       0.403625        0.236518   \n",
       "2      2717      20346      23063       0.547231        0.359871   \n",
       "3      3318      27433      30751       0.668278        0.485222   \n",
       "4      3876      34563      38439       0.780665        0.611334   \n",
       "5      4311      41815      46126       0.868278        0.739604   \n",
       "6      4683      49131      53814       0.943202        0.869006   \n",
       "7      4965      56537      61502       1.000000        1.000000   \n",
       "\n",
       "   total_cum_rate        KS  \n",
       "0        0.125004  0.120362  \n",
       "1        0.250008  0.167108  \n",
       "2        0.374996  0.187360  \n",
       "3        0.500000  0.183056  \n",
       "4        0.625004  0.169330  \n",
       "5        0.749992  0.128674  \n",
       "6        0.874996  0.074196  \n",
       "7        1.000000  0.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "X_train_2_sm, y_train_2_sm = smote.fit_sample(X_train_2, y_train_2)\n",
    "#X_train_2_sm = pd.DataFrame(X_train_2_sm, columns=[X_train_2.columns])\n",
    "\n",
    "model_pipeline_NB_SM.fit(X_train_2_sm,y_train_2_sm)\n",
    "y_pred = model_pipeline_NB_SM.predict(X_test_2)\n",
    "y_pred_proba = model_pipeline_NB_SM.predict_proba(X_test_2)\n",
    "\n",
    "end = time.time()\n",
    "time_model=end-start\n",
    "time_model=time_model/60\n",
    "\n",
    "score_NB_SM, df_NB_SM = modelmetrics('NaiveBayes SM', y_pred, y_pred_proba, y_test_2,time_model) \n",
    "\n",
    "model_compare= pd.concat([model_compare,df_NB_SM])\n",
    "model_compare_NB_SM = model_compare\n",
    "\n",
    "display(model_compare, score_NB_SM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pipeline_TC = make_pipeline(GridSearchCV(DecisionTreeClassifier(random_state=110), \n",
    "                  {'max_depth':[5,10,15,30], \n",
    "                  'min_samples_leaf':[5,10,15]}, \n",
    "                  scoring='roc_auc', \n",
    "                  verbose=1 , \n",
    "                  n_jobs=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:  3.3min finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TP</th>\n",
       "      <th>TN</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Gini</th>\n",
       "      <th>KS</th>\n",
       "      <th>Minutes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>36</td>\n",
       "      <td>56497</td>\n",
       "      <td>40</td>\n",
       "      <td>4929</td>\n",
       "      <td>0.919206</td>\n",
       "      <td>0.007251</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.014283</td>\n",
       "      <td>0.741644</td>\n",
       "      <td>0.483288</td>\n",
       "      <td>0.361981</td>\n",
       "      <td>0.131119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>LogisticRegression RFE</td>\n",
       "      <td>37</td>\n",
       "      <td>56499</td>\n",
       "      <td>38</td>\n",
       "      <td>4928</td>\n",
       "      <td>0.919255</td>\n",
       "      <td>0.007452</td>\n",
       "      <td>0.493333</td>\n",
       "      <td>0.014683</td>\n",
       "      <td>0.742399</td>\n",
       "      <td>0.484797</td>\n",
       "      <td>0.365048</td>\n",
       "      <td>41.746602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>LogisticRegression RFECV</td>\n",
       "      <td>0</td>\n",
       "      <td>56537</td>\n",
       "      <td>0</td>\n",
       "      <td>4965</td>\n",
       "      <td>0.919271</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.539789</td>\n",
       "      <td>0.079578</td>\n",
       "      <td>0.093007</td>\n",
       "      <td>189.196640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>LogisticRegression SM</td>\n",
       "      <td>3226</td>\n",
       "      <td>39654</td>\n",
       "      <td>16883</td>\n",
       "      <td>1739</td>\n",
       "      <td>0.697213</td>\n",
       "      <td>0.649748</td>\n",
       "      <td>0.160426</td>\n",
       "      <td>0.257318</td>\n",
       "      <td>0.735386</td>\n",
       "      <td>0.470772</td>\n",
       "      <td>0.347739</td>\n",
       "      <td>3.915135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NaiveBayes</td>\n",
       "      <td>106</td>\n",
       "      <td>56141</td>\n",
       "      <td>396</td>\n",
       "      <td>4859</td>\n",
       "      <td>0.914556</td>\n",
       "      <td>0.021349</td>\n",
       "      <td>0.211155</td>\n",
       "      <td>0.038778</td>\n",
       "      <td>0.631575</td>\n",
       "      <td>0.263150</td>\n",
       "      <td>0.192399</td>\n",
       "      <td>0.004301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NaiveBayes RFECV SqError</td>\n",
       "      <td>0</td>\n",
       "      <td>56537</td>\n",
       "      <td>0</td>\n",
       "      <td>4965</td>\n",
       "      <td>0.919271</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008951</td>\n",
       "      <td>4.736567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NaiveBayes RFECV AUC</td>\n",
       "      <td>97</td>\n",
       "      <td>56171</td>\n",
       "      <td>366</td>\n",
       "      <td>4868</td>\n",
       "      <td>0.914897</td>\n",
       "      <td>0.019537</td>\n",
       "      <td>0.209503</td>\n",
       "      <td>0.035741</td>\n",
       "      <td>0.631669</td>\n",
       "      <td>0.263338</td>\n",
       "      <td>0.192618</td>\n",
       "      <td>4.469134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NaiveBayes SM</td>\n",
       "      <td>2952</td>\n",
       "      <td>33496</td>\n",
       "      <td>23041</td>\n",
       "      <td>2013</td>\n",
       "      <td>0.592631</td>\n",
       "      <td>0.594562</td>\n",
       "      <td>0.113569</td>\n",
       "      <td>0.190710</td>\n",
       "      <td>0.631469</td>\n",
       "      <td>0.262938</td>\n",
       "      <td>0.187360</td>\n",
       "      <td>1.694798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>TreeClassifier AUC</td>\n",
       "      <td>0</td>\n",
       "      <td>56537</td>\n",
       "      <td>0</td>\n",
       "      <td>4965</td>\n",
       "      <td>0.919271</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.724192</td>\n",
       "      <td>0.448385</td>\n",
       "      <td>0.336785</td>\n",
       "      <td>3.418094</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            TP     TN     FP    FN  Accuracy    Recall  \\\n",
       "LogisticRegression          36  56497     40  4929  0.919206  0.007251   \n",
       "LogisticRegression RFE      37  56499     38  4928  0.919255  0.007452   \n",
       "LogisticRegression RFECV     0  56537      0  4965  0.919271  0.000000   \n",
       "LogisticRegression SM     3226  39654  16883  1739  0.697213  0.649748   \n",
       "NaiveBayes                 106  56141    396  4859  0.914556  0.021349   \n",
       "NaiveBayes RFECV SqError     0  56537      0  4965  0.919271  0.000000   \n",
       "NaiveBayes RFECV AUC        97  56171    366  4868  0.914897  0.019537   \n",
       "NaiveBayes SM             2952  33496  23041  2013  0.592631  0.594562   \n",
       "TreeClassifier AUC           0  56537      0  4965  0.919271  0.000000   \n",
       "\n",
       "                          Precision        F1       AUC      Gini        KS  \\\n",
       "LogisticRegression         0.473684  0.014283  0.741644  0.483288  0.361981   \n",
       "LogisticRegression RFE     0.493333  0.014683  0.742399  0.484797  0.365048   \n",
       "LogisticRegression RFECV   0.000000  0.000000  0.539789  0.079578  0.093007   \n",
       "LogisticRegression SM      0.160426  0.257318  0.735386  0.470772  0.347739   \n",
       "NaiveBayes                 0.211155  0.038778  0.631575  0.263150  0.192399   \n",
       "NaiveBayes RFECV SqError   0.000000  0.000000  0.500000  0.000000  0.008951   \n",
       "NaiveBayes RFECV AUC       0.209503  0.035741  0.631669  0.263338  0.192618   \n",
       "NaiveBayes SM              0.113569  0.190710  0.631469  0.262938  0.187360   \n",
       "TreeClassifier AUC         0.000000  0.000000  0.724192  0.448385  0.336785   \n",
       "\n",
       "                             Minutes  \n",
       "LogisticRegression          0.131119  \n",
       "LogisticRegression RFE     41.746602  \n",
       "LogisticRegression RFECV  189.196640  \n",
       "LogisticRegression SM       3.915135  \n",
       "NaiveBayes                  0.004301  \n",
       "NaiveBayes RFECV SqError    4.736567  \n",
       "NaiveBayes RFECV AUC        4.469134  \n",
       "NaiveBayes SM               1.694798  \n",
       "TreeClassifier AUC          3.418094  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bin</th>\n",
       "      <th>score_min</th>\n",
       "      <th>score_max</th>\n",
       "      <th>bads</th>\n",
       "      <th>total</th>\n",
       "      <th>goods</th>\n",
       "      <th>bad_rate</th>\n",
       "      <th>bads_cum</th>\n",
       "      <th>goods_cum</th>\n",
       "      <th>total_cum</th>\n",
       "      <th>bads_cum_rate</th>\n",
       "      <th>goods_cum_rate</th>\n",
       "      <th>total_cum_rate</th>\n",
       "      <th>KS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>(-0.001, 7687.625]</td>\n",
       "      <td>0.163343</td>\n",
       "      <td>0.370109</td>\n",
       "      <td>1704</td>\n",
       "      <td>7688</td>\n",
       "      <td>5984</td>\n",
       "      <td>0.221644</td>\n",
       "      <td>1704</td>\n",
       "      <td>5984</td>\n",
       "      <td>7688</td>\n",
       "      <td>0.343202</td>\n",
       "      <td>0.105842</td>\n",
       "      <td>0.125004</td>\n",
       "      <td>0.237360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>(7687.625, 15375.25]</td>\n",
       "      <td>0.108174</td>\n",
       "      <td>0.163343</td>\n",
       "      <td>949</td>\n",
       "      <td>7688</td>\n",
       "      <td>6739</td>\n",
       "      <td>0.123439</td>\n",
       "      <td>2653</td>\n",
       "      <td>12723</td>\n",
       "      <td>15376</td>\n",
       "      <td>0.534340</td>\n",
       "      <td>0.225038</td>\n",
       "      <td>0.250008</td>\n",
       "      <td>0.309302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>(15375.25, 23062.875]</td>\n",
       "      <td>0.078892</td>\n",
       "      <td>0.108174</td>\n",
       "      <td>746</td>\n",
       "      <td>7687</td>\n",
       "      <td>6941</td>\n",
       "      <td>0.097047</td>\n",
       "      <td>3399</td>\n",
       "      <td>19664</td>\n",
       "      <td>23063</td>\n",
       "      <td>0.684592</td>\n",
       "      <td>0.347808</td>\n",
       "      <td>0.374996</td>\n",
       "      <td>0.336785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>(23062.875, 30750.5]</td>\n",
       "      <td>0.056797</td>\n",
       "      <td>0.078892</td>\n",
       "      <td>511</td>\n",
       "      <td>7688</td>\n",
       "      <td>7177</td>\n",
       "      <td>0.066467</td>\n",
       "      <td>3910</td>\n",
       "      <td>26841</td>\n",
       "      <td>30751</td>\n",
       "      <td>0.787513</td>\n",
       "      <td>0.474751</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.312762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>(30750.5, 38438.125]</td>\n",
       "      <td>0.044787</td>\n",
       "      <td>0.056797</td>\n",
       "      <td>373</td>\n",
       "      <td>7688</td>\n",
       "      <td>7315</td>\n",
       "      <td>0.048517</td>\n",
       "      <td>4283</td>\n",
       "      <td>34156</td>\n",
       "      <td>38439</td>\n",
       "      <td>0.862638</td>\n",
       "      <td>0.604135</td>\n",
       "      <td>0.625004</td>\n",
       "      <td>0.258503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>(38438.125, 46125.75]</td>\n",
       "      <td>0.042913</td>\n",
       "      <td>0.044787</td>\n",
       "      <td>307</td>\n",
       "      <td>7687</td>\n",
       "      <td>7380</td>\n",
       "      <td>0.039938</td>\n",
       "      <td>4590</td>\n",
       "      <td>41536</td>\n",
       "      <td>46126</td>\n",
       "      <td>0.924471</td>\n",
       "      <td>0.734669</td>\n",
       "      <td>0.749992</td>\n",
       "      <td>0.189802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>(46125.75, 53813.375]</td>\n",
       "      <td>0.024938</td>\n",
       "      <td>0.042913</td>\n",
       "      <td>229</td>\n",
       "      <td>7688</td>\n",
       "      <td>7459</td>\n",
       "      <td>0.029787</td>\n",
       "      <td>4819</td>\n",
       "      <td>48995</td>\n",
       "      <td>53814</td>\n",
       "      <td>0.970594</td>\n",
       "      <td>0.866601</td>\n",
       "      <td>0.874996</td>\n",
       "      <td>0.103994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>(53813.375, 61501.0]</td>\n",
       "      <td>0.017062</td>\n",
       "      <td>0.024938</td>\n",
       "      <td>146</td>\n",
       "      <td>7688</td>\n",
       "      <td>7542</td>\n",
       "      <td>0.018991</td>\n",
       "      <td>4965</td>\n",
       "      <td>56537</td>\n",
       "      <td>61502</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     bin  score_min  score_max  bads  total  goods  bad_rate  \\\n",
       "0     (-0.001, 7687.625]   0.163343   0.370109  1704   7688   5984  0.221644   \n",
       "1   (7687.625, 15375.25]   0.108174   0.163343   949   7688   6739  0.123439   \n",
       "2  (15375.25, 23062.875]   0.078892   0.108174   746   7687   6941  0.097047   \n",
       "3   (23062.875, 30750.5]   0.056797   0.078892   511   7688   7177  0.066467   \n",
       "4   (30750.5, 38438.125]   0.044787   0.056797   373   7688   7315  0.048517   \n",
       "5  (38438.125, 46125.75]   0.042913   0.044787   307   7687   7380  0.039938   \n",
       "6  (46125.75, 53813.375]   0.024938   0.042913   229   7688   7459  0.029787   \n",
       "7   (53813.375, 61501.0]   0.017062   0.024938   146   7688   7542  0.018991   \n",
       "\n",
       "   bads_cum  goods_cum  total_cum  bads_cum_rate  goods_cum_rate  \\\n",
       "0      1704       5984       7688       0.343202        0.105842   \n",
       "1      2653      12723      15376       0.534340        0.225038   \n",
       "2      3399      19664      23063       0.684592        0.347808   \n",
       "3      3910      26841      30751       0.787513        0.474751   \n",
       "4      4283      34156      38439       0.862638        0.604135   \n",
       "5      4590      41536      46126       0.924471        0.734669   \n",
       "6      4819      48995      53814       0.970594        0.866601   \n",
       "7      4965      56537      61502       1.000000        1.000000   \n",
       "\n",
       "   total_cum_rate        KS  \n",
       "0        0.125004  0.237360  \n",
       "1        0.250008  0.309302  \n",
       "2        0.374996  0.336785  \n",
       "3        0.500000  0.312762  \n",
       "4        0.625004  0.258503  \n",
       "5        0.749992  0.189802  \n",
       "6        0.874996  0.103994  \n",
       "7        1.000000  0.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                       max_depth=5, max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=10, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                       random_state=110, splitter='best')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "model_pipeline_TC.fit(X_train_2,y_train_2)\n",
    "y_pred = model_pipeline_TC.predict(X_test_2)\n",
    "y_pred_proba = model_pipeline_TC.predict_proba(X_test_2)\n",
    "\n",
    "end = time.time()\n",
    "time_model=end-start\n",
    "time_model=time_model/60\n",
    "\n",
    "best_model_TC=model_pipeline_TC.named_steps['gridsearchcv'].best_estimator_\n",
    "\n",
    "score_TC, df_TC = modelmetrics('TreeClassifier AUC', y_pred, y_pred_proba, y_test_2,time_model) \n",
    "\n",
    "model_compare=pd.concat([model_compare,df_TC])\n",
    "model_compare_TC = model_compare\n",
    "\n",
    "display(model_compare, score_TC, best_model_TC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'n_estimators': [20,50,100],\n",
    "              'max_depth':[best_model_TC.max_depth],\n",
    "              'min_samples_leaf': [best_model_TC.min_samples_leaf],\n",
    "              'max_features': [10,20,50,80]}\n",
    "\n",
    "model_pipeline_RF = make_pipeline(GridSearchCV(RandomForestClassifier(), \n",
    "                  parameters, \n",
    "                  scoring='roc_auc', \n",
    "                  verbose=1 , \n",
    "                  n_jobs=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed: 12.0min finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TP</th>\n",
       "      <th>TN</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Gini</th>\n",
       "      <th>KS</th>\n",
       "      <th>Minutes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>36</td>\n",
       "      <td>56497</td>\n",
       "      <td>40</td>\n",
       "      <td>4929</td>\n",
       "      <td>0.919206</td>\n",
       "      <td>0.007251</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.014283</td>\n",
       "      <td>0.741644</td>\n",
       "      <td>0.483288</td>\n",
       "      <td>0.361981</td>\n",
       "      <td>0.131119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>LogisticRegression RFE</td>\n",
       "      <td>37</td>\n",
       "      <td>56499</td>\n",
       "      <td>38</td>\n",
       "      <td>4928</td>\n",
       "      <td>0.919255</td>\n",
       "      <td>0.007452</td>\n",
       "      <td>0.493333</td>\n",
       "      <td>0.014683</td>\n",
       "      <td>0.742399</td>\n",
       "      <td>0.484797</td>\n",
       "      <td>0.365048</td>\n",
       "      <td>41.746602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>LogisticRegression RFECV</td>\n",
       "      <td>0</td>\n",
       "      <td>56537</td>\n",
       "      <td>0</td>\n",
       "      <td>4965</td>\n",
       "      <td>0.919271</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.539789</td>\n",
       "      <td>0.079578</td>\n",
       "      <td>0.093007</td>\n",
       "      <td>189.196640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>LogisticRegression SM</td>\n",
       "      <td>3226</td>\n",
       "      <td>39654</td>\n",
       "      <td>16883</td>\n",
       "      <td>1739</td>\n",
       "      <td>0.697213</td>\n",
       "      <td>0.649748</td>\n",
       "      <td>0.160426</td>\n",
       "      <td>0.257318</td>\n",
       "      <td>0.735386</td>\n",
       "      <td>0.470772</td>\n",
       "      <td>0.347739</td>\n",
       "      <td>3.915135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NaiveBayes</td>\n",
       "      <td>106</td>\n",
       "      <td>56141</td>\n",
       "      <td>396</td>\n",
       "      <td>4859</td>\n",
       "      <td>0.914556</td>\n",
       "      <td>0.021349</td>\n",
       "      <td>0.211155</td>\n",
       "      <td>0.038778</td>\n",
       "      <td>0.631575</td>\n",
       "      <td>0.263150</td>\n",
       "      <td>0.192399</td>\n",
       "      <td>0.004301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NaiveBayes RFECV SqError</td>\n",
       "      <td>0</td>\n",
       "      <td>56537</td>\n",
       "      <td>0</td>\n",
       "      <td>4965</td>\n",
       "      <td>0.919271</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008951</td>\n",
       "      <td>4.736567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NaiveBayes RFECV AUC</td>\n",
       "      <td>97</td>\n",
       "      <td>56171</td>\n",
       "      <td>366</td>\n",
       "      <td>4868</td>\n",
       "      <td>0.914897</td>\n",
       "      <td>0.019537</td>\n",
       "      <td>0.209503</td>\n",
       "      <td>0.035741</td>\n",
       "      <td>0.631669</td>\n",
       "      <td>0.263338</td>\n",
       "      <td>0.192618</td>\n",
       "      <td>4.469134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NaiveBayes SM</td>\n",
       "      <td>2952</td>\n",
       "      <td>33496</td>\n",
       "      <td>23041</td>\n",
       "      <td>2013</td>\n",
       "      <td>0.592631</td>\n",
       "      <td>0.594562</td>\n",
       "      <td>0.113569</td>\n",
       "      <td>0.190710</td>\n",
       "      <td>0.631469</td>\n",
       "      <td>0.262938</td>\n",
       "      <td>0.187360</td>\n",
       "      <td>1.694798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>TreeClassifier AUC</td>\n",
       "      <td>0</td>\n",
       "      <td>56537</td>\n",
       "      <td>0</td>\n",
       "      <td>4965</td>\n",
       "      <td>0.919271</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.724192</td>\n",
       "      <td>0.448385</td>\n",
       "      <td>0.336785</td>\n",
       "      <td>3.418094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>RandomForest AUC</td>\n",
       "      <td>0</td>\n",
       "      <td>56537</td>\n",
       "      <td>0</td>\n",
       "      <td>4965</td>\n",
       "      <td>0.919271</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.735861</td>\n",
       "      <td>0.471723</td>\n",
       "      <td>0.355189</td>\n",
       "      <td>13.694479</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            TP     TN     FP    FN  Accuracy    Recall  \\\n",
       "LogisticRegression          36  56497     40  4929  0.919206  0.007251   \n",
       "LogisticRegression RFE      37  56499     38  4928  0.919255  0.007452   \n",
       "LogisticRegression RFECV     0  56537      0  4965  0.919271  0.000000   \n",
       "LogisticRegression SM     3226  39654  16883  1739  0.697213  0.649748   \n",
       "NaiveBayes                 106  56141    396  4859  0.914556  0.021349   \n",
       "NaiveBayes RFECV SqError     0  56537      0  4965  0.919271  0.000000   \n",
       "NaiveBayes RFECV AUC        97  56171    366  4868  0.914897  0.019537   \n",
       "NaiveBayes SM             2952  33496  23041  2013  0.592631  0.594562   \n",
       "TreeClassifier AUC           0  56537      0  4965  0.919271  0.000000   \n",
       "RandomForest AUC             0  56537      0  4965  0.919271  0.000000   \n",
       "\n",
       "                          Precision        F1       AUC      Gini        KS  \\\n",
       "LogisticRegression         0.473684  0.014283  0.741644  0.483288  0.361981   \n",
       "LogisticRegression RFE     0.493333  0.014683  0.742399  0.484797  0.365048   \n",
       "LogisticRegression RFECV   0.000000  0.000000  0.539789  0.079578  0.093007   \n",
       "LogisticRegression SM      0.160426  0.257318  0.735386  0.470772  0.347739   \n",
       "NaiveBayes                 0.211155  0.038778  0.631575  0.263150  0.192399   \n",
       "NaiveBayes RFECV SqError   0.000000  0.000000  0.500000  0.000000  0.008951   \n",
       "NaiveBayes RFECV AUC       0.209503  0.035741  0.631669  0.263338  0.192618   \n",
       "NaiveBayes SM              0.113569  0.190710  0.631469  0.262938  0.187360   \n",
       "TreeClassifier AUC         0.000000  0.000000  0.724192  0.448385  0.336785   \n",
       "RandomForest AUC           0.000000  0.000000  0.735861  0.471723  0.355189   \n",
       "\n",
       "                             Minutes  \n",
       "LogisticRegression          0.131119  \n",
       "LogisticRegression RFE     41.746602  \n",
       "LogisticRegression RFECV  189.196640  \n",
       "LogisticRegression SM       3.915135  \n",
       "NaiveBayes                  0.004301  \n",
       "NaiveBayes RFECV SqError    4.736567  \n",
       "NaiveBayes RFECV AUC        4.469134  \n",
       "NaiveBayes SM               1.694798  \n",
       "TreeClassifier AUC          3.418094  \n",
       "RandomForest AUC           13.694479  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bin</th>\n",
       "      <th>score_min</th>\n",
       "      <th>score_max</th>\n",
       "      <th>bads</th>\n",
       "      <th>total</th>\n",
       "      <th>goods</th>\n",
       "      <th>bad_rate</th>\n",
       "      <th>bads_cum</th>\n",
       "      <th>goods_cum</th>\n",
       "      <th>total_cum</th>\n",
       "      <th>bads_cum_rate</th>\n",
       "      <th>goods_cum_rate</th>\n",
       "      <th>total_cum_rate</th>\n",
       "      <th>KS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>(-0.001, 7687.625]</td>\n",
       "      <td>0.154805</td>\n",
       "      <td>0.327428</td>\n",
       "      <td>1752</td>\n",
       "      <td>7688</td>\n",
       "      <td>5936</td>\n",
       "      <td>0.227888</td>\n",
       "      <td>1752</td>\n",
       "      <td>5936</td>\n",
       "      <td>7688</td>\n",
       "      <td>0.352870</td>\n",
       "      <td>0.104993</td>\n",
       "      <td>0.125004</td>\n",
       "      <td>0.247877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>(7687.625, 15375.25]</td>\n",
       "      <td>0.093837</td>\n",
       "      <td>0.154804</td>\n",
       "      <td>1020</td>\n",
       "      <td>7688</td>\n",
       "      <td>6668</td>\n",
       "      <td>0.132674</td>\n",
       "      <td>2772</td>\n",
       "      <td>12604</td>\n",
       "      <td>15376</td>\n",
       "      <td>0.558308</td>\n",
       "      <td>0.222934</td>\n",
       "      <td>0.250008</td>\n",
       "      <td>0.335375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>(15375.25, 23062.875]</td>\n",
       "      <td>0.073298</td>\n",
       "      <td>0.093836</td>\n",
       "      <td>711</td>\n",
       "      <td>7687</td>\n",
       "      <td>6976</td>\n",
       "      <td>0.092494</td>\n",
       "      <td>3483</td>\n",
       "      <td>19580</td>\n",
       "      <td>23063</td>\n",
       "      <td>0.701511</td>\n",
       "      <td>0.346322</td>\n",
       "      <td>0.374996</td>\n",
       "      <td>0.355189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>(23062.875, 30750.5]</td>\n",
       "      <td>0.060060</td>\n",
       "      <td>0.073297</td>\n",
       "      <td>512</td>\n",
       "      <td>7688</td>\n",
       "      <td>7176</td>\n",
       "      <td>0.066597</td>\n",
       "      <td>3995</td>\n",
       "      <td>26756</td>\n",
       "      <td>30751</td>\n",
       "      <td>0.804632</td>\n",
       "      <td>0.473248</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.331385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>(30750.5, 38438.125]</td>\n",
       "      <td>0.050242</td>\n",
       "      <td>0.060059</td>\n",
       "      <td>362</td>\n",
       "      <td>7688</td>\n",
       "      <td>7326</td>\n",
       "      <td>0.047086</td>\n",
       "      <td>4357</td>\n",
       "      <td>34082</td>\n",
       "      <td>38439</td>\n",
       "      <td>0.877543</td>\n",
       "      <td>0.602826</td>\n",
       "      <td>0.625004</td>\n",
       "      <td>0.274716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>(38438.125, 46125.75]</td>\n",
       "      <td>0.040295</td>\n",
       "      <td>0.050242</td>\n",
       "      <td>254</td>\n",
       "      <td>7687</td>\n",
       "      <td>7433</td>\n",
       "      <td>0.033043</td>\n",
       "      <td>4611</td>\n",
       "      <td>41515</td>\n",
       "      <td>46126</td>\n",
       "      <td>0.928701</td>\n",
       "      <td>0.734298</td>\n",
       "      <td>0.749992</td>\n",
       "      <td>0.194403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>(46125.75, 53813.375]</td>\n",
       "      <td>0.034129</td>\n",
       "      <td>0.040292</td>\n",
       "      <td>230</td>\n",
       "      <td>7688</td>\n",
       "      <td>7458</td>\n",
       "      <td>0.029917</td>\n",
       "      <td>4841</td>\n",
       "      <td>48973</td>\n",
       "      <td>53814</td>\n",
       "      <td>0.975025</td>\n",
       "      <td>0.866212</td>\n",
       "      <td>0.874996</td>\n",
       "      <td>0.108814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>(53813.375, 61501.0]</td>\n",
       "      <td>0.022284</td>\n",
       "      <td>0.034129</td>\n",
       "      <td>124</td>\n",
       "      <td>7688</td>\n",
       "      <td>7564</td>\n",
       "      <td>0.016129</td>\n",
       "      <td>4965</td>\n",
       "      <td>56537</td>\n",
       "      <td>61502</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     bin  score_min  score_max  bads  total  goods  bad_rate  \\\n",
       "0     (-0.001, 7687.625]   0.154805   0.327428  1752   7688   5936  0.227888   \n",
       "1   (7687.625, 15375.25]   0.093837   0.154804  1020   7688   6668  0.132674   \n",
       "2  (15375.25, 23062.875]   0.073298   0.093836   711   7687   6976  0.092494   \n",
       "3   (23062.875, 30750.5]   0.060060   0.073297   512   7688   7176  0.066597   \n",
       "4   (30750.5, 38438.125]   0.050242   0.060059   362   7688   7326  0.047086   \n",
       "5  (38438.125, 46125.75]   0.040295   0.050242   254   7687   7433  0.033043   \n",
       "6  (46125.75, 53813.375]   0.034129   0.040292   230   7688   7458  0.029917   \n",
       "7   (53813.375, 61501.0]   0.022284   0.034129   124   7688   7564  0.016129   \n",
       "\n",
       "   bads_cum  goods_cum  total_cum  bads_cum_rate  goods_cum_rate  \\\n",
       "0      1752       5936       7688       0.352870        0.104993   \n",
       "1      2772      12604      15376       0.558308        0.222934   \n",
       "2      3483      19580      23063       0.701511        0.346322   \n",
       "3      3995      26756      30751       0.804632        0.473248   \n",
       "4      4357      34082      38439       0.877543        0.602826   \n",
       "5      4611      41515      46126       0.928701        0.734298   \n",
       "6      4841      48973      53814       0.975025        0.866212   \n",
       "7      4965      56537      61502       1.000000        1.000000   \n",
       "\n",
       "   total_cum_rate        KS  \n",
       "0        0.125004  0.247877  \n",
       "1        0.250008  0.335375  \n",
       "2        0.374996  0.355189  \n",
       "3        0.500000  0.331385  \n",
       "4        0.625004  0.274716  \n",
       "5        0.749992  0.194403  \n",
       "6        0.874996  0.108814  \n",
       "7        1.000000  0.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=5, max_features=80,\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=10, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "model_pipeline_RF.fit(X_train_2,y_train_2)\n",
    "y_pred = model_pipeline_RF.predict(X_test_2)\n",
    "y_pred_proba = model_pipeline_RF.predict_proba(X_test_2)\n",
    "\n",
    "end = time.time()\n",
    "time_model=end-start\n",
    "time_model=time_model/60\n",
    "\n",
    "best_model_RF=model_pipeline_RF.named_steps['gridsearchcv'].best_estimator_\n",
    "\n",
    "score_RF, df_RF = modelmetrics('RandomForest AUC', y_pred, y_pred_proba, y_test_2,time_model) \n",
    "\n",
    "model_compare=pd.concat([model_compare,df_RF])\n",
    "model_compare_RF = model_compare\n",
    "\n",
    "display(model_compare, score_RF, best_model_RF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'n_estimators': [20,50,100],\n",
    "              'max_depth':[best_model_TC.max_depth],\n",
    "              'min_samples_leaf': [best_model_TC.min_samples_leaf],\n",
    "              'max_features': [10,20,50,80]}\n",
    "\n",
    "model_pipeline_ET = make_pipeline(GridSearchCV(ExtraTreesClassifier(), \n",
    "                  parameters, \n",
    "                  scoring='roc_auc', \n",
    "                  verbose=1 , \n",
    "                  n_jobs=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed: 10.2min finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TP</th>\n",
       "      <th>TN</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Gini</th>\n",
       "      <th>KS</th>\n",
       "      <th>Minutes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>36</td>\n",
       "      <td>56497</td>\n",
       "      <td>40</td>\n",
       "      <td>4929</td>\n",
       "      <td>0.919206</td>\n",
       "      <td>0.007251</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.014283</td>\n",
       "      <td>0.741644</td>\n",
       "      <td>0.483288</td>\n",
       "      <td>0.361981</td>\n",
       "      <td>0.131119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>LogisticRegression RFE</td>\n",
       "      <td>37</td>\n",
       "      <td>56499</td>\n",
       "      <td>38</td>\n",
       "      <td>4928</td>\n",
       "      <td>0.919255</td>\n",
       "      <td>0.007452</td>\n",
       "      <td>0.493333</td>\n",
       "      <td>0.014683</td>\n",
       "      <td>0.742399</td>\n",
       "      <td>0.484797</td>\n",
       "      <td>0.365048</td>\n",
       "      <td>41.746602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>LogisticRegression RFECV</td>\n",
       "      <td>0</td>\n",
       "      <td>56537</td>\n",
       "      <td>0</td>\n",
       "      <td>4965</td>\n",
       "      <td>0.919271</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.539789</td>\n",
       "      <td>0.079578</td>\n",
       "      <td>0.093007</td>\n",
       "      <td>189.196640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>LogisticRegression SM</td>\n",
       "      <td>3226</td>\n",
       "      <td>39654</td>\n",
       "      <td>16883</td>\n",
       "      <td>1739</td>\n",
       "      <td>0.697213</td>\n",
       "      <td>0.649748</td>\n",
       "      <td>0.160426</td>\n",
       "      <td>0.257318</td>\n",
       "      <td>0.735386</td>\n",
       "      <td>0.470772</td>\n",
       "      <td>0.347739</td>\n",
       "      <td>3.915135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NaiveBayes</td>\n",
       "      <td>106</td>\n",
       "      <td>56141</td>\n",
       "      <td>396</td>\n",
       "      <td>4859</td>\n",
       "      <td>0.914556</td>\n",
       "      <td>0.021349</td>\n",
       "      <td>0.211155</td>\n",
       "      <td>0.038778</td>\n",
       "      <td>0.631575</td>\n",
       "      <td>0.263150</td>\n",
       "      <td>0.192399</td>\n",
       "      <td>0.004301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NaiveBayes RFECV SqError</td>\n",
       "      <td>0</td>\n",
       "      <td>56537</td>\n",
       "      <td>0</td>\n",
       "      <td>4965</td>\n",
       "      <td>0.919271</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008951</td>\n",
       "      <td>4.736567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NaiveBayes RFECV AUC</td>\n",
       "      <td>97</td>\n",
       "      <td>56171</td>\n",
       "      <td>366</td>\n",
       "      <td>4868</td>\n",
       "      <td>0.914897</td>\n",
       "      <td>0.019537</td>\n",
       "      <td>0.209503</td>\n",
       "      <td>0.035741</td>\n",
       "      <td>0.631669</td>\n",
       "      <td>0.263338</td>\n",
       "      <td>0.192618</td>\n",
       "      <td>4.469134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NaiveBayes SM</td>\n",
       "      <td>2952</td>\n",
       "      <td>33496</td>\n",
       "      <td>23041</td>\n",
       "      <td>2013</td>\n",
       "      <td>0.592631</td>\n",
       "      <td>0.594562</td>\n",
       "      <td>0.113569</td>\n",
       "      <td>0.190710</td>\n",
       "      <td>0.631469</td>\n",
       "      <td>0.262938</td>\n",
       "      <td>0.187360</td>\n",
       "      <td>1.694798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>TreeClassifier AUC</td>\n",
       "      <td>0</td>\n",
       "      <td>56537</td>\n",
       "      <td>0</td>\n",
       "      <td>4965</td>\n",
       "      <td>0.919271</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.724192</td>\n",
       "      <td>0.448385</td>\n",
       "      <td>0.336785</td>\n",
       "      <td>3.418094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>RandomForest AUC</td>\n",
       "      <td>0</td>\n",
       "      <td>56537</td>\n",
       "      <td>0</td>\n",
       "      <td>4965</td>\n",
       "      <td>0.919271</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.735861</td>\n",
       "      <td>0.471723</td>\n",
       "      <td>0.355189</td>\n",
       "      <td>13.694479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>ExtraTrees AUC</td>\n",
       "      <td>0</td>\n",
       "      <td>56537</td>\n",
       "      <td>0</td>\n",
       "      <td>4965</td>\n",
       "      <td>0.919271</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.734088</td>\n",
       "      <td>0.468176</td>\n",
       "      <td>0.351902</td>\n",
       "      <td>11.478524</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            TP     TN     FP    FN  Accuracy    Recall  \\\n",
       "LogisticRegression          36  56497     40  4929  0.919206  0.007251   \n",
       "LogisticRegression RFE      37  56499     38  4928  0.919255  0.007452   \n",
       "LogisticRegression RFECV     0  56537      0  4965  0.919271  0.000000   \n",
       "LogisticRegression SM     3226  39654  16883  1739  0.697213  0.649748   \n",
       "NaiveBayes                 106  56141    396  4859  0.914556  0.021349   \n",
       "NaiveBayes RFECV SqError     0  56537      0  4965  0.919271  0.000000   \n",
       "NaiveBayes RFECV AUC        97  56171    366  4868  0.914897  0.019537   \n",
       "NaiveBayes SM             2952  33496  23041  2013  0.592631  0.594562   \n",
       "TreeClassifier AUC           0  56537      0  4965  0.919271  0.000000   \n",
       "RandomForest AUC             0  56537      0  4965  0.919271  0.000000   \n",
       "ExtraTrees AUC               0  56537      0  4965  0.919271  0.000000   \n",
       "\n",
       "                          Precision        F1       AUC      Gini        KS  \\\n",
       "LogisticRegression         0.473684  0.014283  0.741644  0.483288  0.361981   \n",
       "LogisticRegression RFE     0.493333  0.014683  0.742399  0.484797  0.365048   \n",
       "LogisticRegression RFECV   0.000000  0.000000  0.539789  0.079578  0.093007   \n",
       "LogisticRegression SM      0.160426  0.257318  0.735386  0.470772  0.347739   \n",
       "NaiveBayes                 0.211155  0.038778  0.631575  0.263150  0.192399   \n",
       "NaiveBayes RFECV SqError   0.000000  0.000000  0.500000  0.000000  0.008951   \n",
       "NaiveBayes RFECV AUC       0.209503  0.035741  0.631669  0.263338  0.192618   \n",
       "NaiveBayes SM              0.113569  0.190710  0.631469  0.262938  0.187360   \n",
       "TreeClassifier AUC         0.000000  0.000000  0.724192  0.448385  0.336785   \n",
       "RandomForest AUC           0.000000  0.000000  0.735861  0.471723  0.355189   \n",
       "ExtraTrees AUC             0.000000  0.000000  0.734088  0.468176  0.351902   \n",
       "\n",
       "                             Minutes  \n",
       "LogisticRegression          0.131119  \n",
       "LogisticRegression RFE     41.746602  \n",
       "LogisticRegression RFECV  189.196640  \n",
       "LogisticRegression SM       3.915135  \n",
       "NaiveBayes                  0.004301  \n",
       "NaiveBayes RFECV SqError    4.736567  \n",
       "NaiveBayes RFECV AUC        4.469134  \n",
       "NaiveBayes SM               1.694798  \n",
       "TreeClassifier AUC          3.418094  \n",
       "RandomForest AUC           13.694479  \n",
       "ExtraTrees AUC             11.478524  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bin</th>\n",
       "      <th>score_min</th>\n",
       "      <th>score_max</th>\n",
       "      <th>bads</th>\n",
       "      <th>total</th>\n",
       "      <th>goods</th>\n",
       "      <th>bad_rate</th>\n",
       "      <th>bads_cum</th>\n",
       "      <th>goods_cum</th>\n",
       "      <th>total_cum</th>\n",
       "      <th>bads_cum_rate</th>\n",
       "      <th>goods_cum_rate</th>\n",
       "      <th>total_cum_rate</th>\n",
       "      <th>KS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>(-0.001, 7687.625]</td>\n",
       "      <td>0.124661</td>\n",
       "      <td>0.306012</td>\n",
       "      <td>1761</td>\n",
       "      <td>7688</td>\n",
       "      <td>5927</td>\n",
       "      <td>0.229058</td>\n",
       "      <td>1761</td>\n",
       "      <td>5927</td>\n",
       "      <td>7688</td>\n",
       "      <td>0.354683</td>\n",
       "      <td>0.104834</td>\n",
       "      <td>0.125004</td>\n",
       "      <td>0.249849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>(7687.625, 15375.25]</td>\n",
       "      <td>0.096401</td>\n",
       "      <td>0.124657</td>\n",
       "      <td>999</td>\n",
       "      <td>7688</td>\n",
       "      <td>6689</td>\n",
       "      <td>0.129943</td>\n",
       "      <td>2760</td>\n",
       "      <td>12616</td>\n",
       "      <td>15376</td>\n",
       "      <td>0.555891</td>\n",
       "      <td>0.223146</td>\n",
       "      <td>0.250008</td>\n",
       "      <td>0.332745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>(15375.25, 23062.875]</td>\n",
       "      <td>0.081696</td>\n",
       "      <td>0.096391</td>\n",
       "      <td>708</td>\n",
       "      <td>7687</td>\n",
       "      <td>6979</td>\n",
       "      <td>0.092104</td>\n",
       "      <td>3468</td>\n",
       "      <td>19595</td>\n",
       "      <td>23063</td>\n",
       "      <td>0.698489</td>\n",
       "      <td>0.346587</td>\n",
       "      <td>0.374996</td>\n",
       "      <td>0.351902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>(23062.875, 30750.5]</td>\n",
       "      <td>0.070915</td>\n",
       "      <td>0.081696</td>\n",
       "      <td>496</td>\n",
       "      <td>7688</td>\n",
       "      <td>7192</td>\n",
       "      <td>0.064516</td>\n",
       "      <td>3964</td>\n",
       "      <td>26787</td>\n",
       "      <td>30751</td>\n",
       "      <td>0.798389</td>\n",
       "      <td>0.473796</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.324593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>(30750.5, 38438.125]</td>\n",
       "      <td>0.061317</td>\n",
       "      <td>0.070914</td>\n",
       "      <td>375</td>\n",
       "      <td>7688</td>\n",
       "      <td>7313</td>\n",
       "      <td>0.048777</td>\n",
       "      <td>4339</td>\n",
       "      <td>34100</td>\n",
       "      <td>38439</td>\n",
       "      <td>0.873917</td>\n",
       "      <td>0.603145</td>\n",
       "      <td>0.625004</td>\n",
       "      <td>0.270773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>(38438.125, 46125.75]</td>\n",
       "      <td>0.052440</td>\n",
       "      <td>0.061315</td>\n",
       "      <td>265</td>\n",
       "      <td>7687</td>\n",
       "      <td>7422</td>\n",
       "      <td>0.034474</td>\n",
       "      <td>4604</td>\n",
       "      <td>41522</td>\n",
       "      <td>46126</td>\n",
       "      <td>0.927291</td>\n",
       "      <td>0.734422</td>\n",
       "      <td>0.749992</td>\n",
       "      <td>0.192869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>(46125.75, 53813.375]</td>\n",
       "      <td>0.042611</td>\n",
       "      <td>0.052439</td>\n",
       "      <td>213</td>\n",
       "      <td>7688</td>\n",
       "      <td>7475</td>\n",
       "      <td>0.027706</td>\n",
       "      <td>4817</td>\n",
       "      <td>48997</td>\n",
       "      <td>53814</td>\n",
       "      <td>0.970191</td>\n",
       "      <td>0.866636</td>\n",
       "      <td>0.874996</td>\n",
       "      <td>0.103555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>(53813.375, 61501.0]</td>\n",
       "      <td>0.025679</td>\n",
       "      <td>0.042610</td>\n",
       "      <td>148</td>\n",
       "      <td>7688</td>\n",
       "      <td>7540</td>\n",
       "      <td>0.019251</td>\n",
       "      <td>4965</td>\n",
       "      <td>56537</td>\n",
       "      <td>61502</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     bin  score_min  score_max  bads  total  goods  bad_rate  \\\n",
       "0     (-0.001, 7687.625]   0.124661   0.306012  1761   7688   5927  0.229058   \n",
       "1   (7687.625, 15375.25]   0.096401   0.124657   999   7688   6689  0.129943   \n",
       "2  (15375.25, 23062.875]   0.081696   0.096391   708   7687   6979  0.092104   \n",
       "3   (23062.875, 30750.5]   0.070915   0.081696   496   7688   7192  0.064516   \n",
       "4   (30750.5, 38438.125]   0.061317   0.070914   375   7688   7313  0.048777   \n",
       "5  (38438.125, 46125.75]   0.052440   0.061315   265   7687   7422  0.034474   \n",
       "6  (46125.75, 53813.375]   0.042611   0.052439   213   7688   7475  0.027706   \n",
       "7   (53813.375, 61501.0]   0.025679   0.042610   148   7688   7540  0.019251   \n",
       "\n",
       "   bads_cum  goods_cum  total_cum  bads_cum_rate  goods_cum_rate  \\\n",
       "0      1761       5927       7688       0.354683        0.104834   \n",
       "1      2760      12616      15376       0.555891        0.223146   \n",
       "2      3468      19595      23063       0.698489        0.346587   \n",
       "3      3964      26787      30751       0.798389        0.473796   \n",
       "4      4339      34100      38439       0.873917        0.603145   \n",
       "5      4604      41522      46126       0.927291        0.734422   \n",
       "6      4817      48997      53814       0.970191        0.866636   \n",
       "7      4965      56537      61502       1.000000        1.000000   \n",
       "\n",
       "   total_cum_rate        KS  \n",
       "0        0.125004  0.249849  \n",
       "1        0.250008  0.332745  \n",
       "2        0.374996  0.351902  \n",
       "3        0.500000  0.324593  \n",
       "4        0.625004  0.270773  \n",
       "5        0.749992  0.192869  \n",
       "6        0.874996  0.103555  \n",
       "7        1.000000  0.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,\n",
       "                     criterion='gini', max_depth=5, max_features=80,\n",
       "                     max_leaf_nodes=None, max_samples=None,\n",
       "                     min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                     min_samples_leaf=10, min_samples_split=2,\n",
       "                     min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                     n_jobs=None, oob_score=False, random_state=None, verbose=0,\n",
       "                     warm_start=False)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "model_pipeline_ET.fit(X_train_2,y_train_2)\n",
    "y_pred = model_pipeline_ET.predict(X_test_2)\n",
    "y_pred_proba = model_pipeline_ET.predict_proba(X_test_2)\n",
    "\n",
    "end = time.time()\n",
    "time_model=end-start\n",
    "time_model=time_model/60\n",
    "\n",
    "best_model_ET=model_pipeline_ET.named_steps['gridsearchcv'].best_estimator_\n",
    "\n",
    "score_ET, df_ET = modelmetrics('ExtraTrees AUC', y_pred, y_pred_proba, y_test_2,time_model) \n",
    "\n",
    "model_compare=pd.concat([model_compare,df_ET])\n",
    "model_compare_ET = model_compare\n",
    "\n",
    "display(model_compare, score_ET, best_model_ET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data set\n",
    "\n",
    "model_compare.to_csv('model_compare.csv')\n",
    "score_LR.to_csv('score_LR.csv')\n",
    "score_LR_RFE.to_csv('score_LR_RFE.csv')\n",
    "score_LR_RFECV.to_csv('score_LR_RFECV.csv')\n",
    "score_LR_SM.to_csv('score_LR_SM.csv')\n",
    "score_NB.to_csv('score_NB.csv')\n",
    "score_NB_RFECV.to_csv('score_NB_RFECV.csv')\n",
    "score_NB_RFECV_2.to_csv('score_NB_RFECV_2.csv')\n",
    "score_NB_SM.to_csv('score_SM.csv')\n",
    "score_TC.to_csv('score_TC.csv')\n",
    "score_RF.to_csv('score_RF.csv')\n",
    "score_ET.to_csv('score_ET.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TP</th>\n",
       "      <th>TN</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>F1</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Gini</th>\n",
       "      <th>KS</th>\n",
       "      <th>Minutes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>36</td>\n",
       "      <td>56497</td>\n",
       "      <td>40</td>\n",
       "      <td>4929</td>\n",
       "      <td>0.919206</td>\n",
       "      <td>0.007251</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.014283</td>\n",
       "      <td>0.741644</td>\n",
       "      <td>0.483288</td>\n",
       "      <td>0.361981</td>\n",
       "      <td>0.131119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>LogisticRegression RFE</td>\n",
       "      <td>37</td>\n",
       "      <td>56499</td>\n",
       "      <td>38</td>\n",
       "      <td>4928</td>\n",
       "      <td>0.919255</td>\n",
       "      <td>0.007452</td>\n",
       "      <td>0.493333</td>\n",
       "      <td>0.014683</td>\n",
       "      <td>0.742399</td>\n",
       "      <td>0.484797</td>\n",
       "      <td>0.365048</td>\n",
       "      <td>41.746602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>LogisticRegression RFECV</td>\n",
       "      <td>0</td>\n",
       "      <td>56537</td>\n",
       "      <td>0</td>\n",
       "      <td>4965</td>\n",
       "      <td>0.919271</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.539789</td>\n",
       "      <td>0.079578</td>\n",
       "      <td>0.093007</td>\n",
       "      <td>189.196640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>LogisticRegression SM</td>\n",
       "      <td>3226</td>\n",
       "      <td>39654</td>\n",
       "      <td>16883</td>\n",
       "      <td>1739</td>\n",
       "      <td>0.697213</td>\n",
       "      <td>0.649748</td>\n",
       "      <td>0.160426</td>\n",
       "      <td>0.257318</td>\n",
       "      <td>0.735386</td>\n",
       "      <td>0.470772</td>\n",
       "      <td>0.347739</td>\n",
       "      <td>3.915135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NaiveBayes</td>\n",
       "      <td>106</td>\n",
       "      <td>56141</td>\n",
       "      <td>396</td>\n",
       "      <td>4859</td>\n",
       "      <td>0.914556</td>\n",
       "      <td>0.021349</td>\n",
       "      <td>0.211155</td>\n",
       "      <td>0.038778</td>\n",
       "      <td>0.631575</td>\n",
       "      <td>0.263150</td>\n",
       "      <td>0.192399</td>\n",
       "      <td>0.004301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NaiveBayes RFECV SqError</td>\n",
       "      <td>0</td>\n",
       "      <td>56537</td>\n",
       "      <td>0</td>\n",
       "      <td>4965</td>\n",
       "      <td>0.919271</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008951</td>\n",
       "      <td>4.736567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NaiveBayes RFECV AUC</td>\n",
       "      <td>97</td>\n",
       "      <td>56171</td>\n",
       "      <td>366</td>\n",
       "      <td>4868</td>\n",
       "      <td>0.914897</td>\n",
       "      <td>0.019537</td>\n",
       "      <td>0.209503</td>\n",
       "      <td>0.035741</td>\n",
       "      <td>0.631669</td>\n",
       "      <td>0.263338</td>\n",
       "      <td>0.192618</td>\n",
       "      <td>4.469134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>NaiveBayes SM</td>\n",
       "      <td>2952</td>\n",
       "      <td>33496</td>\n",
       "      <td>23041</td>\n",
       "      <td>2013</td>\n",
       "      <td>0.592631</td>\n",
       "      <td>0.594562</td>\n",
       "      <td>0.113569</td>\n",
       "      <td>0.190710</td>\n",
       "      <td>0.631469</td>\n",
       "      <td>0.262938</td>\n",
       "      <td>0.187360</td>\n",
       "      <td>1.694798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>TreeClassifier AUC</td>\n",
       "      <td>0</td>\n",
       "      <td>56537</td>\n",
       "      <td>0</td>\n",
       "      <td>4965</td>\n",
       "      <td>0.919271</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.724192</td>\n",
       "      <td>0.448385</td>\n",
       "      <td>0.336785</td>\n",
       "      <td>3.418094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>RandomForest AUC</td>\n",
       "      <td>0</td>\n",
       "      <td>56537</td>\n",
       "      <td>0</td>\n",
       "      <td>4965</td>\n",
       "      <td>0.919271</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.735861</td>\n",
       "      <td>0.471723</td>\n",
       "      <td>0.355189</td>\n",
       "      <td>13.694479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>ExtraTrees AUC</td>\n",
       "      <td>0</td>\n",
       "      <td>56537</td>\n",
       "      <td>0</td>\n",
       "      <td>4965</td>\n",
       "      <td>0.919271</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.734088</td>\n",
       "      <td>0.468176</td>\n",
       "      <td>0.351902</td>\n",
       "      <td>11.478524</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            TP     TN     FP    FN  Accuracy    Recall  \\\n",
       "LogisticRegression          36  56497     40  4929  0.919206  0.007251   \n",
       "LogisticRegression RFE      37  56499     38  4928  0.919255  0.007452   \n",
       "LogisticRegression RFECV     0  56537      0  4965  0.919271  0.000000   \n",
       "LogisticRegression SM     3226  39654  16883  1739  0.697213  0.649748   \n",
       "NaiveBayes                 106  56141    396  4859  0.914556  0.021349   \n",
       "NaiveBayes RFECV SqError     0  56537      0  4965  0.919271  0.000000   \n",
       "NaiveBayes RFECV AUC        97  56171    366  4868  0.914897  0.019537   \n",
       "NaiveBayes SM             2952  33496  23041  2013  0.592631  0.594562   \n",
       "TreeClassifier AUC           0  56537      0  4965  0.919271  0.000000   \n",
       "RandomForest AUC             0  56537      0  4965  0.919271  0.000000   \n",
       "ExtraTrees AUC               0  56537      0  4965  0.919271  0.000000   \n",
       "\n",
       "                          Precision        F1       AUC      Gini        KS  \\\n",
       "LogisticRegression         0.473684  0.014283  0.741644  0.483288  0.361981   \n",
       "LogisticRegression RFE     0.493333  0.014683  0.742399  0.484797  0.365048   \n",
       "LogisticRegression RFECV   0.000000  0.000000  0.539789  0.079578  0.093007   \n",
       "LogisticRegression SM      0.160426  0.257318  0.735386  0.470772  0.347739   \n",
       "NaiveBayes                 0.211155  0.038778  0.631575  0.263150  0.192399   \n",
       "NaiveBayes RFECV SqError   0.000000  0.000000  0.500000  0.000000  0.008951   \n",
       "NaiveBayes RFECV AUC       0.209503  0.035741  0.631669  0.263338  0.192618   \n",
       "NaiveBayes SM              0.113569  0.190710  0.631469  0.262938  0.187360   \n",
       "TreeClassifier AUC         0.000000  0.000000  0.724192  0.448385  0.336785   \n",
       "RandomForest AUC           0.000000  0.000000  0.735861  0.471723  0.355189   \n",
       "ExtraTrees AUC             0.000000  0.000000  0.734088  0.468176  0.351902   \n",
       "\n",
       "                             Minutes  \n",
       "LogisticRegression          0.131119  \n",
       "LogisticRegression RFE     41.746602  \n",
       "LogisticRegression RFECV  189.196640  \n",
       "LogisticRegression SM       3.915135  \n",
       "NaiveBayes                  0.004301  \n",
       "NaiveBayes RFECV SqError    4.736567  \n",
       "NaiveBayes RFECV AUC        4.469134  \n",
       "NaiveBayes SM               1.694798  \n",
       "TreeClassifier AUC          3.418094  \n",
       "RandomForest AUC           13.694479  \n",
       "ExtraTrees AUC             11.478524  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_compare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best performing model: Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in a model that can best separate the target class from the non-target class. \n",
    "In order to compare the different models’ performance, we look at the AUC metric (Area Under the Curve).\n",
    "There are four models with an AUC equal to 0.74. Two of them have the highest Gini coefficient, that is Logistic Regression and Logistic Regression using RFE (recursive feature elimination). However, since the former’s running time is considerably lower, it is chosen as the best performing model for this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAEWCAYAAACHVDePAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3dd5xU1f3/8dd7d0XABqhgwRbFgv7sGmILliCWBHs0xhYT1GhijRpj7EZiYv3aYowKGguxEoMiQbEX7Eo0glhAEYwUBZWyfH5/3LM4rLuzs7i7szPzfu7jPnbm3HPPPXdn9nPPnHvmXEUEZmZW3qqKXQEzM2t9DvZmZhXAwd7MrAI42JuZVQAHezOzCuBgb2ZWARzsW4CkTpL+KWmmpH98i3IOlvRwS9atGCQ9KOmwYtejUJKuk/T7xdhudUmzJFW3Rr3as3Tc3yl2PaxwFRXsJf1E0gvpjTo5BaXtWqDo/YAewPIRsf/iFhIRf4+Ifi1Qn0VI6ispJN1TL32TlD66wHLOkXRrU/kiYreIGLyY1c23/zVTfWtastyIODoizi9g/+9J2iVnuw8iYumIqG3O/iQdLqk2vQ8/k/SqpD0Xp+7Fko57QrHrYYWrmGAv6STgcuAPZIF5deAaYEALFL8G8HZEzG+BslrLJ8A2kpbPSTsMeLuldqBMxbynvqVnImJpoAvZ+/AOSV1aeictfWK0EhYRZb8AywGzgP3z5FmS7GTwUVouB5ZM6/oCk4CTganAZOCItO5cYC4wL+3jSOAc4NacstcEAqhJzw8HJgCfA+8CB+ekP5mz3TbAGGBm+r1NzrrRwPnAU6mch4EVGjm2uvpfBxyb0qpT2lnA6Jy8VwATgc+AF4HtU3r/esf5ak49Lkz1+BJYJ6X9PK2/Frgrp/w/AqMALcbruMjfsdDXL60/Nb1uHwE/T+Wsk9bdDFyQHq8APADMAKYBT5A1im4BFqRjnJXKq/+6dgNuSvuYDtzXyHHUf507p3K2yknrAzyd6vEq0Ddn3VrA4+l1/zdwNen9llOnI4EPgMcLKO9wGn4/rgM8Rvb++x9wZ842uX+/5YAhZA2K94EzgarcYwX+nP4m7wK7FTsmVOJS9Aq0yUFmgWp+Q0EiJ895wLNAd2DF9I9xflrXN21/HrAEsDvwBdA1rT+HRYN7/ecLgwKwFFkgXS+tWxnYMD1eGARS4JgOHJK2Oyg9Xz6tHw28A6wLdErPBzVybH3JAvs2wHMpbXdgBFngG52T96fA8mmfJwMfAx0bOq6cenwAbJi2WYJFg31nsk8PhwPbp6DRczFfx4V/x2a+fv3TcWyY6nMLjQf7i8hOikukZXvSiQl4D9ilsfoA/wLuBLqmbb/fyHHkvs7VwLFkJ9LuKW1V4NP0GlUBP0jPV0zrnyELnh2A7cjeT/WD/RCy91qnfOWR//14O/C7tE1HYLucY8j9+w0B7geWSft/Gzgy51jnAb9Ix3oM2cmw2Sd7L99uqZSP3MsD/4v83SwHA+dFxNSI+ISsxX5Izvp5af28iBhO1rpbbzHrswDYSFKniJgcEWMbyLMHMC4ibomI+RFxO/AW8MOcPDdFxNsR8SUwFNg0304j4mmgm6T1gEPJ/knr57k1Ij5N+7yErMXc1HHeHBFj0zbz6pX3BdkJ5FLgVuBXETGpifIWR77X7wCyv9XYVJ9z85QzjyzgrZFe6yciRa18JK0M7AYcHRHT07aP5dmkj6QZwFdkgfunETE1rfspMDwihkfEgogYCbwA7C5pdWAr4KyImBsRTwLDGij/nIiYnd4bjZaX8jb2fpxH1kW5SkR8lfZV/7irgR8Dv42IzyPiPeASFv3feT8i/hrZtY3BZH/fHnn+NtYKKiXYfwqs0ET/5SpkH0HrvJ/SFpZR72TxBbB0cysSEbPJ/jmOBiZL+pek9QuoT12dVs15/vFi1OcW4DhgR+De+islnSzpzTSyaAbZR/QVmihzYr6VEfE8WTeByE5KDZI0Nl20nCVp+yb2WV++12+VenXMV98/AeOBhyVNkHR6gftfDZgWEdMLzP9sRHQh+xQwjOwTRJ01gP0lzahbyFrwK6djmZZOWvmOJzet0fKaeD+eSvaaPZ9em581sJ8VyD5h1P/bN/g+zal3s/937NuplGD/DFkLaq88eT4i+6eos3pKWxyzyboL6qyUuzIiRkTED8j+ed8C/lpAferq9OFi1qnOLcAvyVp6uQGDFGBPI2sJd03BaCbZPzxkH90bkrflK+lYsk8IH5EFkIYLidgwslEeS0fEE4UcTI58r99koGfOutXy1OHziDg5Ir5D9inqJEk7163Os/+JZJ+amnWRNSJmkb0eh0jaLKesWyKiS86yVEQMSsfSTVLu+6uh48mta77yGn0/RsTHEfGLiFgFOAq4RtI69fbzP77+BFCnJd6n1sIqIthHxEyyC5FXS9pLUmdJS0jaTdLFKdvtwJmSVpS0Qsrf5DDDRrwC7JDGYS8H/LZuhaQekn4kaSlgDll3UEND94YD66bhojWSfgz0Jrt4uNgi4l3g+2R9sfUtQ3Zt4hOgRtJZwLI566cAazZnxI2kdYELyLoSDgFOlZS3u6kAS0rqmLNUkf/1GwocIWmDFCTPylPfPSWtI0lkfdm1fP36TAEaHFseEZOBB8kCYtf0/tqhkIOJiE+BG3LqdSvwQ0m7SqpOx9hXUs+IeJ+sC+YcSR0kfY9Fu/Ya0mh5+d6PkvaXVHeSnE52AlnkvZq6ZoYCF0paRtIawEks/v+OtZKKCPYAEXEp2ZvwTLJgNpGsO+O+lOUCsn+i14DXgZdS2uLsayTZhbrXyEa05AboKrILnx+Rjfb4PlnLrn4ZnwJ7pryfkrWI94yI/y1OneqV/WRENPSpZQRZwHqb7KP4VyzaHVD3hbFPJb3U1H5St9mtwB8j4tWIGAecAdwiaclvcQizyEbF1C07kef1i4gHgSuBR8m6aJ5J5cxpoOxeZCNcZqV810TE6LTuIrITygxJpzSw7SFkrdy3yEZtndCMY7qcrE9+44iYSDYk+Ay+fq/+hq//Xw8Gvkf2vriA7L3W0LEA0ER5+d6PWwHPSZpF1tV0fGos1Pcrsk+zE8hG3twG3NiMY7c2UDfKwKxiSNoAeINsaGZ7/m5EQSTdCbwVEWcXuy7WflVMy94qm6S9U7dHV7Kx/v8s1UAvaStJa0uqktSfrNV+X1PbWWVzsLdKcRRZF8Y7ZP3OxxS3Ot/KSmTfZZhF1j11TES8XNQaWbvnbhwzswrglr2ZWQVot5MkddrsOH/ksG+YPuaqYlfB2qGONQu/C7LYmhNzvnz5qm+9v7bmlr2ZWQVoty17M7M2VeazczvYm5kBVJX3Dccc7M3MAFRy3fDN4mBvZgbuxjEzqwhu2ZuZVQC37M3MKoBb9mZmFcCjcczMKoC7cczMKoC7cczMKoBb9mZmFcDB3sysAlT7Aq2ZWflzn72ZWQVwN46ZWQVwy97MrAK4ZW9mVgHcsjczqwCeLsHMrAKUeTdOeR+dmVmhpMKXJovSe5Jel/SKpBdSWjdJIyWNS7+7pnRJulLSeEmvSdo8p5zDUv5xkg7LSd8ilT8+bdtkpRzszcwga9kXuhRmx4jYNCK2TM9PB0ZFRC9gVHoOsBvQKy0DgWshOzkAZwPfBbYGzq47QaQ8A3O2699UZRzszcygNYJ9fQOAwenxYGCvnPQhkXkW6CJpZWBXYGRETIuI6cBIoH9at2xEPBMRAQzJKatRDvZmZpBdoC1wkTRQ0gs5y8B6pQXwsKQXc9b1iIjJAOl395S+KjAxZ9tJKS1f+qQG0vPyBVozM2jW0MuIuB64Pk+WbSPiI0ndgZGS3sq354Z2sRjpebllb2YGLdqNExEfpd9TgXvJ+tynpC4Y0u+pKfskYLWczXsCHzWR3rOB9Lwc7M3MoMVG40haStIydY+BfsAbwDCgbkTNYcD96fEw4NA0KqcPMDN184wA+knqmi7M9gNGpHWfS+qTRuEcmlNWo9yNY2YGFDB6sVA9gHtTeTXAbRHxkKQxwFBJRwIfAPun/MOB3YHxwBfAEQARMU3S+cCYlO+8iJiWHh8D3Ax0Ah5MS14O9mZmtFywj4gJwCYNpH8K7NxAegDHNlLWjcCNDaS/AGzUnHo52JuZAary3DhmZmWvBbtx2iUHezMzHOzNzCqCg72ZWSUo71jvYG9mBm7Zm5lVhKqq8v6OqYO9mRlu2ZuZVYbyjvUO9mZm4Ja9mVlFcLA3M6sAni7BzKwCuGVvZlYBHOzNzCqAg72ZWQVwsDczqwTlHesd7M3MwNMlmJlVBHfjmJlVgvKO9Q72bemtf53L57PnULtgAfNrF7DdwRcDcMyB3+foH+/A/NoFPPTEG/zuivsXbrPaSl156e4zufC64Vx+yygAjj2oL0fssw2SuOmep7jqttEA3DLoCHqt2QOALst0YsbnX9LnwEFteozWempraznogH3p3qMHV13zFyZNmshpp5zEZzNnsn7v3vzhootZokOHYlezZLllby2q/8Ar+HTG7IXPd9iyF3v2/X9sdcBFzJ03nxW7Lr1I/otP2ZeHnxq78HnvtVfmiH22YftD/sTcebUMu/qXPPjkWN754BMOOf2mhfkGnbQ3M2d92foHZG3m77cM4TvfWZtZs2cBcMWlf+anhx7ObrvvwfnnnsW999zFAQf+pMi1LF3lHuxb7YqEpPUlnSbpSklXpMcbtNb+StXA/bfnzzeNZO68+QB8Mn3WwnU/7Lsx7076H/955+OFaeuvtRLPv/4eX341j9raBTzx4ngG7LjJN8rd9webM/ShF1v/AKxNTPn4Y554fDR777sfABHB8889yw/67QrAjwbszSOjRhWziiVPUsFLKWqVYC/pNOAOsl6w54Ex6fHtkk5vjX2Wgojgn9ccx1N/P5Wf7bMtAOus0Z1tN1ubx4ecwsM3HM8WvVcHoHPHDpx8xA+48C/DFylj7Dsfsd3m69BtuaXo1HEJ+m+3IT1X6rpInm03X5sp0z7nnQ8+aZsDs1Z38aA/cOLJv1k4YmTGjOkss8yy1NRkH8579FiJqVOnFLOKJU9VKngpRa3VjXMksGFEzMtNlHQpMBZosCNZ0kBgIEBNz77UrLBhK1WvOHY64jImfzKTFbsuzQPXHcd/3/uYmuoqui7bmR0O/TNbbrgGt178MzbY8xx+f8we/N+tjzD7y7mLlPHfd6dwyc0jeeDa45j95Rxee/tD5s+vXSTPAf235B8PvdCWh2at6LHRj9KtWzd6b7gRY55/DoCIb+Yr1RZne1Huf7/WCvYLgFWA9+ulr5zWNSgirgeuB+i02XENvJ1L2+RPZgJZV82wR15jqw3X5MMpM7hv1KsAvDD2fRYsCFboujRbbbQGe++yKReesBfLLdOJBQuCr+bO47o7H2fwfc8w+L5nADj3uB/y4ZQZC/dRXV3FgJ02YdufXNz2B2it4pWXX2L06Ed48onHmTNnDrNnz+JPgy7k888/Y/78+dTU1DBlysesuGL3Yle1pDnYL54TgFGSxgETU9rqwDrAca20z3atc8cOVFWJWV/MoXPHDuzyvfX5w/UPMuvLOfTdel2eeHEc66zenQ5L1PC/6bPY5cjLF277u6N2Z/YXc7juzscBWLHr0nwyfRarrdSVATttQt/DLlmYd6fvrsfb703hw6kzvlEHK03Hn3gyx594MgBjnn+OwTffyEUXX8IpJ/6akQ+PYLfd92DY/fey4047Fbmmpa3MY33rBPuIeEjSusDWwKpk/fWTgDERUZt34zLVfflluPPSXwBQU13NnQ++wMin32SJmmr+cs7BvPCPM5g7r5afn3VLk2Xd/uef063LUsybX8sJg4Yy4/OvR93sv+sWvjBbIU446TecesqJXH3l5ay/wQbsve/+xa5SSSv3lr2ioc6/dqAcu3Hs25s+5qpiV8HaoY413/4rUeudNqLgmPPfP+5acmcGj7M3M6P8u3HKe+YfM7MCVVWp4KUQkqolvSzpgfR8LUnPSRon6U5JHVL6kun5+LR+zZwyfpvS/ytp15z0/iltfKHD2R3szczIWvaFLgU6Hngz5/kfgcsiohcwnWyIOun39IhYB7gs5UNSb+BAYEOgP3BNOoFUA1cDuwG9gYNS3rwc7M3MaNlv0ErqCewB3JCeC9gJuCtlGQzslR4PSM9J63dO+QcAd0TEnIh4FxhPNuhla2B8REyIiLlkX2Ad0FSdHOzNzGhey17SQEkv5CwD6xV3OXAqX3+vaHlgRkTMT88nkY1UJP2eCJDWz0z5F6bX26ax9Lx8gdbMjObdvCT3C6D1SdoTmBoRL0rqW5fcUDFNrGssvaGKNjmSyMHezIwWHY2zLfAjSbsDHYFlyVr6XSTVpNZ7T+CjlH8SsBowSVINsBwwLSe9Tu42jaU3yt04Zma0XJ99RPw2InpGxJpkF1gfiYiDgUeB/VK2w4C6G1cMS89J6x+J7AtQw4AD02idtYBefD2xZK80uqdD2sewpo7PLXszM9pknP1pwB2SLgBeBv6W0v8G3CJpPFmL/kCAiBgraSjwH2A+cGzdDASSjgNGANXAjRExliY42JuZ0TrTJUTEaGB0ejyBbCRN/TxfAQ3OdRERFwIXNpA+HBj+zS0a52BvZkb5f4PWwd7MDAr+ZmypcrA3M6P8Z710sDczw904ZmYVwS17M7MKUOax3sHezAx8gdbMrCK4G8fMrAI42JuZVYAyj/UO9mZm4Ja9mVlFKPNY72BvZgYejWNmVhGqyrxpX9DNSyQdKOl36fFqkrZo3WqZmbWt5tyDthQ1GewlXQXsCPw0Jc0GrmvNSpmZtbWWulNVe1VIN842EbG5pJcBImJauhWWmVnZKPMu+4KC/TxJVaS7l0taHljQqrUyM2tj5X6BtpA++6uBu4EVJZ0LPAn8sVVrZWbWxtSMn1LUZMs+IoZIehHYBRCwf0S80eo1MzNrQ2XesG862EtaBZgO/CM3LSI+as2KmZm1pVK98FqoQvrsR5H664FOwGrAO8B6rVUpM7O2VuaxvqBunA1yn0vaGjii1WpkZlYE/lJVPRHxPLB1K9TFzKxoqqpU8FKKCumz/3XO0ypgC2Baq9XIzKwIyrxhX1Cf/Yo5j+cD/ybnYq2ZWTko926cQvrsf98WFTEzK6byDvV5gr2ke/l6FM43RMQ+rVIjM7MiqOShl1e1WS3MzIqsRK+7FqzRYB8Ro9qyImZmxdRSo2wkdQQeB5Yki7F3RcTZktYC7gC6AS8Bh0TEXElLAkPIBr98Cvw4It5LZf0WOBKoBX4dESNSen/gCqAauCEiBjV5fAVUfG1Jd0h6TdLbdUszj9/MrF1rwSmO5wA7RcQmwKZAf0l9yOYUuywiepHNSnBkyn8kMD0i1gEuS/mQ1Bs4ENgQ6A9cI6laUjXZnGW7Ab2Bg1LevAoZZ38zcBPZ9YvdgKFkZyczs7JRpcKXfCIzKz1dIi0B7ATcldIHA3ulxwPSc9L6nZWdUQYAd0TEnIh4FxhP9h2nrYHxETEhIuaSxeMBTR5fAX+DznUfHSLinYg4k+xmJmZmZaM5LXtJAyW9kLMMrFdWtaRXgKnASLIpZmZExPyUZRKwanq8KjARIK2fCSyfm15vm8bS8ypknP2cdJZ5R9LRwIdA9wK2MzMrGc3psY+I64Hr86yvBTaV1AW4F9igoWx5dh150htqpDc6crJOIcH+RGBp4NfAhcCywM8K2M7MrGRUt8JwnIiYIWk00AfoIqkmtd57AnUzB08im2BykqQaYDmyWQrq0uvkbtNYeqMK6cb5IiI+j4gPIuKQiBgQEU8VsJ2ZWcloqQu0klZMLXokdSK7F8ibwKPAfinbYcD96fGw9Jy0/pGIiJR+oKQl00ieXsDzwBigl6S10i1iD0x58yqkZX+NpG5kF2bvjIi3CtjGzKyktOB3qlYGBqdRM1XA0Ih4QNJ/gDskXQC8DPwt5f8bcIuk8WQt+gMBImKspKHAf8imqjk2dQ8h6ThgBNnQyxsjYmxTlSpkuoTtJa0K/DgdQAeyoN/kuE4zs1LRUnPjRMRrwGYNpE+ggRmDI+IrYP9GyrqQrPu8fvpwYHhz6lXQFMcR8WFEXAocDrwOnN+cnZiZtXdS4UspKmSK415krfr9gFnAncBprVwvJoy+tLV3YWa2UCXPjVPnNrJB+z+KiA9auT5mZkVRXenBPiK2aouKmJkVU8VOhGZmVkkc7M3MKoD77BNJS0bEnNasjJlZsZR7y76QKY63lvQ6MC4930TS/7V6zczM2lC5D70sZJz9lcCeZJPqExGv4lkvzazM1EgFL6WokG6cqoh4v15/Vm0r1cfMrChKNIYXrJBgP1HS1kCkuR5+BfhOVWZWVlpquoT2qpBgfwxZV87qwBTg3ynNzKxslHmsL+hLVVNJs7CZmZWrch+NU8jcOH+lgbugRMTABrKbmZWk1rh5SXtSSDfOv3MedwT2ZtH7H5qZlbwyj/UFdePcmftc0i1kN9A1MysbatZdaEvP4kyXsBawRktXxMysmCq+ZS9pOl/32VeR3Tbr9NaslJlZW6voYK/sm1SbAB+mpAXpRrhmZmWloidCi4iQdG9EbNFWFTIzK4bqgm7SWroKObznJW3e6jUxMyuiKqngpRQ12rKXVBMR84HtgF9IegeYDYis0e8TgJmVjUrus38e2BzYq43qYmZWNCXaYC9YvmAvgIh4p43qYmZWNFUVPM5+RUknNbYyIi5thfqYmRVFJbfsq4GlocxPd2ZmQE2Zd9rnC/aTI+K8NquJmVkRVXLLvswP3czsa6U6pLJQ+YL9zm1WCzOzIivzWN94sI+IaW1ZETOzYirzL9CW/fGZmRWkpb5BK2k1SY9KelPSWEnHp/RukkZKGpd+d03pknSlpPGSXsudsUDSYSn/OEmH5aRvIen1tM2VKmBiHwd7MzNadLqE+cDJEbEB0Ac4VlJvstmCR0VEL2AUX88evBvQKy0DgWshOzkAZwPfBbYGzq47QaQ8A3O269/k8RX4dzAzK2tqxpJPREyOiJfS48+BN4FVgQHA4JRtMF/PTjAAGBKZZ4EuklYGdgVGRsS0iJhOdtOo/mndshHxTJqFeAgFzHTgYG9mRnaBtvBFAyW9kLM0eE9uSWsCmwHPAT0iYjJkJwSge8q2Kove6nVSSsuXPqmB9LwW505VZmZlpznz2UfE9cD1TZS3NHA3cEJEfJan/IZWxGKk5+WWvZkZWTAsdGmKpCXIAv3fI+KelDwldcGQfk9N6ZOA1XI27wl81ER6zwbSmzw+M7OK14KjcQT8DXiz3hxiw4C6ETWHAffnpB+aRuX0AWambp4RQD9JXdOF2X7AiLTuc0l90r4OzSmrUe7GMTOjRW9LuC1wCPC6pFdS2hnAIGCopCOBD4D907rhwO7AeOAL4AjIvusk6XxgTMp3Xs73n44BbgY6AQ+mJS+111vKTp45t31WzIqq61Idil0Fa4c61nz76V3ueXVywTFnn01WLrnv27plb2ZGhd9w3MysUpR3qHewNzMDoNotezOz8lfmsd7B3swMQGXekeNgb2aGW/ZmZhWhyi17M7Py55a9mVkFqOR70JqZVYyq8o71DvZmZuDROGZmFaHMe3Ec7IuptraWow47kBVW7M6gy67mpTHPce2VlzBv3jzWW783vznzXGpqahj50APcPuRGADp16syJp/2eddZdD4C77riVB+67GyLYY6992f+gQ4p5SNZK5syZwxGHHsy8uXOZX1vLD/rtyi+P+zURwVVXXs7DIx6iurqK/X98EAf/9NBiV7ckuWVvrebuO25ljTXXYvbs2SxYsICLzv0dl159A6utsSY3/uUqRvxrGHsM2IeVV+nJFdfdxDLLLsdzTz/BJRedy7U33caEd8bxwH13c93Nt1FTswSnHn8039t2B3quvkaxD81aWIcOHbjhxsF0Xmop5s2bx+GH/ITttt+BCRPe4eOPJ3P/Aw9SVVXFp59+Wuyqlqxy77P3zUuKZOqUj3n2qSfYY8C+AHw2cwZLdOjAamusCcCWW3+Pxx8dCcBGG2/KMssuB0DvjTbmk6lTAPjg3Qn03mhjOnbsRE1NDZtuviVPjB7V9gdjrU4SnZdaCoD58+czf/58kBh6x+0cdfSxVFVl/8rLL798MatZ0lrq5iXtlYN9kVx12cUc9asTUfonXa5LV2rnz+et/4wF4LFHRjJ1ysff2O5fw+5l6+9tB8Baa/fitZdfZOaMGXz11Zc8+9QTDW5j5aG2tpYD9hnAjttvQ5/vbcPGG2/CpIkTGfHQcA46YB9+edTPef/994pdzZKlZiylqM2DvaQj8qxbeMf2W2++oS2r1aaefuIxunbtxnobbLgwTRJnXXAxV192MUcffhCdOnemunrRXraXX3ie4cPu4ajjTgRgjbW+w0GH/oxTfjWQU399NGv3Wo/q6uo2PRZrO9XV1Qy9534efuQx3nj9NcaNe5u5c+fSYckluX3oPeyz3wGcfeYZxa5mySr3ln2b36lK0gcRsXpT+cr5TlXXX305Dz/4T6qra5g7Zw5fzJ7N9jvuzJnnDVqYZ8yzT/Ov++/mnIsuAeCdcf/l96eewB8vv3ZhV099f73mClbs3oO99juwLQ6jKHynqsx111xFp06duOfuf3DNX25g1VV7EhFs12dLnnruxWJXr821xJ2qnh0/o+CY02edLiUX8VulZS/ptUaW14EerbHPUjLw2BO464FR3Hn/CM668E9stuXWnHneIKZPyy6uzZ07l9uH3MiP9jkAgCkfT+b3p53IGede9I1AX7fNlI8n8/ij/2bnfru16bFY25g2bRqfffYZAF999RXPPvM0a671HXbcaReef+5ZAF4Y8zxrNNIQsAKUeT9Oa43G6QHsCkyvly7g6VbaZ8m749abeebJx4gFwY/2PYDNt/ouAINvuI7PZs7gsj9eAGQf568fcicAZ512Ep99NoOa6hpO+M3vFl7ItfLyv0+mcuYZp7NgQS0LFgT9du3P9/vuyGabb8EZp53CrUMG07lzZ84+78JiV7VklWr3TKFapRtH0t+AmyLiyQbW3RYRP2mqjHLuxrHF524ca0hLdOOMmTCz4Jiz1XeWK6Fqbx8AAAdKSURBVLkzQ6u07CPiyDzrmgz0ZmZtruTCd/P4S1VmZvgbtGZmFaHMu+wd7M3MoOx7cRzszcwg+2JjOXOwNzPD3ThmZhWhzGO9g72ZGVD20d6zXpqZkQ29LPSnybKkGyVNlfRGTlo3SSMljUu/u6Z0SbpS0vg0rczmOdsclvKPk3RYTvoWkl5P21ypAi44ONibmZH12Re6FOBmoH+9tNOBURHRCxiVngPsBvRKy0Dg2qw+6gacDXwX2Bo4u+4EkfIMzNmu/r6+wcHezIyWDfYR8TgwrV7yAGBwejwY2CsnfUhkngW6SFqZbH6xkRExLSKmAyOB/mndshHxTGTz3QzJKatRDvZmZjSvGyf33htpGVjALnpExGSA9Lt7Sl8VmJiTb1JKy5c+qYH0vHyB1syM5g29jIjrgetbatcN7WIx0vNyy97MjDaZzn5K6oIh/Z6a0icBq+Xk6wl81ER6zwbS83KwNzODtoj2w4C6ETWHAffnpB+aRuX0AWambp4RQD9JXdOF2X7AiLTuc0l90iicQ3PKapS7cczMaNmbl0i6HegLrCBpEtmomkHAUElHAh8A+6fsw4HdgfHAF8ARABExTdL5wJiU77yIqLvoewzZiJ9OwINpyV+ntr4HbaF88xJriG9eYg1piZuXvP3xFwXHnHVX6lxyX8Fyy97MDMr+G7QO9mZm+OYlZmYVwbNemplVgDKP9Q72Zmbgm5eYmVWEMo/1DvZmZuBuHDOzylDm0d7B3swMD700M6sI7rM3M6sAVQ72ZmaVoLyjvYO9mRnuxjEzqwhlHusd7M3MwC17M7OK4OkSzMwqQHmHegd7MzPA3ThmZhXB36A1M6sE5R3rHezNzKDsY72DvZkZQFWZd9o72JuZUf4XaKuKXQEzM2t9btmbmVH+LXsHezMzPPTSzKwiuGVvZlYBHOzNzCqAu3HMzCqAW/ZmZhWgzGO9g72ZGVD20d7B3syM8p8uQRFR7DpYEyQNjIjri10Pa1/8vrDm8HQJpWFgsStg7ZLfF1YwB3szswrgYG9mVgEc7EuD+2WtIX5fWMF8gdbMrAK4ZW9mVgEc7M3MKoCDfTsnqb+k/0oaL+n0YtfHik/SjZKmSnqj2HWx0uFg345JqgauBnYDegMHSepd3FpZO3Az0L/YlbDS4mDfvm0NjI+ICRExF7gDGFDkOlmRRcTjwLRi18NKi4N9+7YqMDHn+aSUZmbWLA727VtDMzN5rKyZNZuDffs2CVgt53lP4KMi1cXMSpiDffs2BuglaS1JHYADgWFFrpOZlSAH+3YsIuYDxwEjgDeBoRExtri1smKTdDvwDLCepEmSjix2naz983QJZmYVwC17M7MK4GBvZlYBHOzNzCqAg72ZWQVwsDczqwAO9tYgSbWSXpH0hqR/SOr8LcrqK+mB9PhH+WbvlNRF0i8XYx/nSDplcevY0uWYtTcO9taYLyNi04jYCJgLHJ27Uplmv38iYlhEDMqTpQvQ7GBvZvk52FshngDWkbSmpDclXQO8BKwmqZ+kZyS9lD4BLA0L5+F/S9KTwD51BUk6XNJV6XEPSfdKejUt2wCDgLXTp4o/pXy/kTRG0muSzs0p63dprv9/A+vVr7Sk5SS9V3dSktRZ0kRJS0j6RSrzVUl3N/TJRdJoSVumxytIei89rpb0p5w6HdUyf2az1uNgb3lJqiGbT//1lLQeMCQiNgNmA2cCu0TE5sALwEmSOgJ/BX4IbA+s1EjxVwKPRcQmwObAWOB04J30qeI3kvoBvcime94U2ELSDpK2IJs+YjOyk8lW9QuPiJnAq8D3U9IPgRERMQ+4JyK2Svt+E2jOt1CPBGZGxFZpv7+QtFYztjdrczXFroC1W50kvZIePwH8DVgFeD8ink3pfchuqvKUJIAOZF/jXx94NyLGAUi6FRjYwD52Ag4FiIhaYKakrvXy9EvLy+n50mTBfxng3oj4Iu2jsTmD7gR+DDxKdnK4JqVvJOkCsm6jpcmmpChUP2BjSful58ulOr3bjDLM2pSDvTXmy4jYNDchBfTZuUnAyIg4qF6+TWm5qZgFXBQRf6m3jxMK3Mcw4CJJ3YAtgEdS+s3AXhHxqqTDgb4NbDufrz/9dqxXp19FRHNOEGZF5W4c+zaeBbaVtA4s7BNfF3gLWEvS2infQY1sPwo4Jm1bLWlZ4HOyVnudEcDPcq4FrCqpO/A4sLekTpKWIeui+YaImAU8D1wBPJA+QZD2MVnSEsDBjdTvPbITBMB+OekjgGPStkhaV9JSjZRh1i442Ntii4hPgMOB2yW9Rhb814+Ir8i6bf6VLtC+30gRxwM7SnodeBHYMCI+JesWekPSnyLiYeA24JmU7y5gmYh4iayL5hXgbrKupsbcCfw0/a7ze+A5YCTZyakhfyYL6k8DK+Sk3wD8B3gp3fT7L/hTsrVznvXSzKwCuGVvZlYBHOzNzCqAg72ZWQVwsDczqwAO9mZmFcDB3sysAjjYm5lVgP8PokY1Ntt2gDEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cmx = [[df_LR.iloc[0,1], df_LR.iloc[0,2]], [df_LR.iloc[0,3], df_LR.iloc[0,0]]]\n",
    "ax = sns.heatmap(cmx , annot=True,fmt='d', cmap='Blues')\n",
    "\n",
    "plt.xlabel('Predicted value')\n",
    "plt.ylabel('True value')\n",
    "plt.title('Confusion Matrix - Logistic Regression')\n",
    "\n",
    "ax.set_ylim(2.0, 0) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bin</th>\n",
       "      <th>score_min</th>\n",
       "      <th>score_max</th>\n",
       "      <th>bads</th>\n",
       "      <th>total</th>\n",
       "      <th>goods</th>\n",
       "      <th>bad_rate</th>\n",
       "      <th>bads_cum</th>\n",
       "      <th>goods_cum</th>\n",
       "      <th>total_cum</th>\n",
       "      <th>bads_cum_rate</th>\n",
       "      <th>goods_cum_rate</th>\n",
       "      <th>total_cum_rate</th>\n",
       "      <th>KS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>(-0.001, 7687.625]</td>\n",
       "      <td>0.154064</td>\n",
       "      <td>0.672522</td>\n",
       "      <td>1748</td>\n",
       "      <td>7688</td>\n",
       "      <td>5940</td>\n",
       "      <td>0.227367</td>\n",
       "      <td>1748</td>\n",
       "      <td>5940</td>\n",
       "      <td>7688</td>\n",
       "      <td>0.352064</td>\n",
       "      <td>0.105064</td>\n",
       "      <td>0.125004</td>\n",
       "      <td>0.247001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>(7687.625, 15375.25]</td>\n",
       "      <td>0.102452</td>\n",
       "      <td>0.154051</td>\n",
       "      <td>1035</td>\n",
       "      <td>7688</td>\n",
       "      <td>6653</td>\n",
       "      <td>0.134625</td>\n",
       "      <td>2783</td>\n",
       "      <td>12593</td>\n",
       "      <td>15376</td>\n",
       "      <td>0.560524</td>\n",
       "      <td>0.222739</td>\n",
       "      <td>0.250008</td>\n",
       "      <td>0.337785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>(15375.25, 23062.875]</td>\n",
       "      <td>0.075157</td>\n",
       "      <td>0.102446</td>\n",
       "      <td>731</td>\n",
       "      <td>7687</td>\n",
       "      <td>6956</td>\n",
       "      <td>0.095096</td>\n",
       "      <td>3514</td>\n",
       "      <td>19549</td>\n",
       "      <td>23063</td>\n",
       "      <td>0.707754</td>\n",
       "      <td>0.345774</td>\n",
       "      <td>0.374996</td>\n",
       "      <td>0.361981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>(23062.875, 30750.5]</td>\n",
       "      <td>0.057100</td>\n",
       "      <td>0.075156</td>\n",
       "      <td>495</td>\n",
       "      <td>7688</td>\n",
       "      <td>7193</td>\n",
       "      <td>0.064386</td>\n",
       "      <td>4009</td>\n",
       "      <td>26742</td>\n",
       "      <td>30751</td>\n",
       "      <td>0.807452</td>\n",
       "      <td>0.473000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.334452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>(30750.5, 38438.125]</td>\n",
       "      <td>0.043729</td>\n",
       "      <td>0.057098</td>\n",
       "      <td>368</td>\n",
       "      <td>7688</td>\n",
       "      <td>7320</td>\n",
       "      <td>0.047867</td>\n",
       "      <td>4377</td>\n",
       "      <td>34062</td>\n",
       "      <td>38439</td>\n",
       "      <td>0.881571</td>\n",
       "      <td>0.602473</td>\n",
       "      <td>0.625004</td>\n",
       "      <td>0.279098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>(38438.125, 46125.75]</td>\n",
       "      <td>0.032831</td>\n",
       "      <td>0.043729</td>\n",
       "      <td>291</td>\n",
       "      <td>7687</td>\n",
       "      <td>7396</td>\n",
       "      <td>0.037856</td>\n",
       "      <td>4668</td>\n",
       "      <td>41458</td>\n",
       "      <td>46126</td>\n",
       "      <td>0.940181</td>\n",
       "      <td>0.733290</td>\n",
       "      <td>0.749992</td>\n",
       "      <td>0.206892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>(46125.75, 53813.375]</td>\n",
       "      <td>0.022683</td>\n",
       "      <td>0.032829</td>\n",
       "      <td>186</td>\n",
       "      <td>7688</td>\n",
       "      <td>7502</td>\n",
       "      <td>0.024194</td>\n",
       "      <td>4854</td>\n",
       "      <td>48960</td>\n",
       "      <td>53814</td>\n",
       "      <td>0.977644</td>\n",
       "      <td>0.865982</td>\n",
       "      <td>0.874996</td>\n",
       "      <td>0.111662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>(53813.375, 61501.0]</td>\n",
       "      <td>0.001559</td>\n",
       "      <td>0.022682</td>\n",
       "      <td>111</td>\n",
       "      <td>7688</td>\n",
       "      <td>7577</td>\n",
       "      <td>0.014438</td>\n",
       "      <td>4965</td>\n",
       "      <td>56537</td>\n",
       "      <td>61502</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     bin  score_min  score_max  bads  total  goods  bad_rate  \\\n",
       "0     (-0.001, 7687.625]   0.154064   0.672522  1748   7688   5940  0.227367   \n",
       "1   (7687.625, 15375.25]   0.102452   0.154051  1035   7688   6653  0.134625   \n",
       "2  (15375.25, 23062.875]   0.075157   0.102446   731   7687   6956  0.095096   \n",
       "3   (23062.875, 30750.5]   0.057100   0.075156   495   7688   7193  0.064386   \n",
       "4   (30750.5, 38438.125]   0.043729   0.057098   368   7688   7320  0.047867   \n",
       "5  (38438.125, 46125.75]   0.032831   0.043729   291   7687   7396  0.037856   \n",
       "6  (46125.75, 53813.375]   0.022683   0.032829   186   7688   7502  0.024194   \n",
       "7   (53813.375, 61501.0]   0.001559   0.022682   111   7688   7577  0.014438   \n",
       "\n",
       "   bads_cum  goods_cum  total_cum  bads_cum_rate  goods_cum_rate  \\\n",
       "0      1748       5940       7688       0.352064        0.105064   \n",
       "1      2783      12593      15376       0.560524        0.222739   \n",
       "2      3514      19549      23063       0.707754        0.345774   \n",
       "3      4009      26742      30751       0.807452        0.473000   \n",
       "4      4377      34062      38439       0.881571        0.602473   \n",
       "5      4668      41458      46126       0.940181        0.733290   \n",
       "6      4854      48960      53814       0.977644        0.865982   \n",
       "7      4965      56537      61502       1.000000        1.000000   \n",
       "\n",
       "   total_cum_rate        KS  \n",
       "0        0.125004  0.247001  \n",
       "1        0.250008  0.337785  \n",
       "2        0.374996  0.361981  \n",
       "3        0.500000  0.334452  \n",
       "4        0.625004  0.279098  \n",
       "5        0.749992  0.206892  \n",
       "6        0.874996  0.111662  \n",
       "7        1.000000  0.000000  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-S is a measure of the degree of separation between the positive and negative distributions.\n",
    "After Logistic Regression with RFE model, Logistic Regression is the model that best separates default cases from non-default cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "score_LR_plot = score_LR\n",
    "score_LR_plot.plot(x = 'total_cum_rate',y= ['bads_cum_rate', 'goods_cum_rate'], \n",
    "              marker= 'o',\n",
    "              markerfacecolor='blue', linestyle='dashed', markersize=8, \n",
    "              color=[ 'red','skyblue'], linewidth=2, figsize = (10,6), \n",
    "              title =  'KS graph - Logistic Regression', rot= 90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next chart shows the distribution of the positive (non-target) class.\n",
    "It is possible to observe that the percentage of non-target cases over the total cases decreases as the predicted probability of default lowers. In other words, actual defaults are more associated with high scores (high probability of default) than with low scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "score_LR_plot['bucket']= score_LR_plot.index\n",
    "score_LR_plot['bucket'] =score_LR_plot['bucket']+1\n",
    "\n",
    "plt.bar(x = score_LR_plot['bucket'],height= score_LR_plot['bad_rate'], width=0.5, align='center')\n",
    "plt.xlabel('Bucket', fontsize=10)\n",
    "plt.ylabel('Bad Rate', fontsize=10)\n",
    "plt.xticks(score_LR_plot['bucket'], label=score_LR_plot['bucket'], fontsize=10)\n",
    "plt.axhline(score_LR_plot['bad_rate'].mean(), color='r')\n",
    "plt.title('Bad Rate')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
